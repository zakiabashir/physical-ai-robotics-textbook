"use strict";(globalThis.webpackChunkphysical_ai_robotics_textbook=globalThis.webpackChunkphysical_ai_robotics_textbook||[]).push([[1402],{3623:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>f,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapter-1/lesson-2","title":"Lesson 1.2: Sensors & Perception Systems","description":"Understanding how robots perceive and understand their environment","source":"@site/docs/chapter-1/lesson-2.mdx","sourceDirName":"chapter-1","slug":"/chapter-1/lesson-2","permalink":"/ur/docs/chapter-1/lesson-2","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai/physical-ai-robotics-textbook/tree/main/docs/chapter-1/lesson-2.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Lesson 1.2: Sensors & Perception Systems","description":"Understanding how robots perceive and understand their environment","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1.1: What is Physical AI?","permalink":"/ur/docs/chapter-1/lesson-1"},"next":{"title":"Lesson 1.3: Weekly Learning Plan (Weeks 1-2)","permalink":"/ur/docs/chapter-1/lesson-3"}}');var t=s(4848),r=s(8453),o=s(2948);const a={title:"Lesson 1.2: Sensors & Perception Systems",description:"Understanding how robots perceive and understand their environment",sidebar_position:2},l="Lesson 1.2: Sensors & Perception Systems",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Types of Sensors",id:"types-of-sensors",level:2},{value:"1. Vision Sensors",id:"1-vision-sensors",level:3},{value:"Cameras",id:"cameras",level:4},{value:"2. Depth Sensors",id:"2-depth-sensors",level:3},{value:"LiDAR (Light Detection and Ranging)",id:"lidar-light-detection-and-ranging",level:4},{value:"3. Proximity Sensors",id:"3-proximity-sensors",level:3},{value:"Ultrasonic Sensors",id:"ultrasonic-sensors",level:4},{value:"4. Inertial Sensors",id:"4-inertial-sensors",level:3},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",level:4},{value:"5. Specialized Sensors",id:"5-specialized-sensors",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:4},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Why Fuse Sensors?",id:"why-fuse-sensors",level:3},{value:"Fusion Techniques",id:"fusion-techniques",level:3},{value:"Perception Pipeline",id:"perception-pipeline",level:2},{value:"End-to-End Perception System",id:"end-to-end-perception-system",level:3},{value:"Lab Exercise: Building a Simple Perception System",id:"lab-exercise-building-a-simple-perception-system",level:2},{value:"Objective",id:"objective",level:3},{value:"Setup",id:"setup",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Expected Output",id:"expected-output",level:3},{value:"Extension Exercises",id:"extension-exercises",level:3},{value:"Challenges in Robot Perception",id:"challenges-in-robot-perception",level:2},{value:"1. Environmental Variability",id:"1-environmental-variability",level:3},{value:"2. Sensor Limitations",id:"2-sensor-limitations",level:3},{value:"3. Real-Time Constraints",id:"3-real-time-constraints",level:3},{value:"4. Ambiguity and Uncertainty",id:"4-ambiguity-and-uncertainty",level:3},{value:"5. Scale and Range",id:"5-scale-and-range",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Summary",id:"summary",level:2},{value:"Quiz",id:"quiz",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components},{DiagramComponent:s,Quiz:i}=n;return s||u("DiagramComponent",!0),i||u("Quiz",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-12-sensors--perception-systems",children:"Lesson 1.2: Sensors & Perception Systems"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)("div",{className:"learning-objectives",children:[(0,t.jsx)(n.p,{children:"After completing this lesson, you will be able to:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Explain the role of sensors in Physical AI systems"}),"\n",(0,t.jsx)(n.li,{children:"Identify different types of sensors used in robotics"}),"\n",(0,t.jsx)(n.li,{children:"Understand sensor fusion and data processing"}),"\n",(0,t.jsx)(n.li,{children:"Implement basic perception algorithms"}),"\n",(0,t.jsx)(n.li,{children:"Recognize challenges in robot perception"}),"\n"]})]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Perception is the foundation of Physical AI. Just as humans use their senses to understand the world, robots use sensors to gather information about their environment. These sensors serve as the robot's eyes, ears, and sense of touch, enabling it to make informed decisions and take appropriate actions."}),"\n",(0,t.jsx)(n.h2,{id:"types-of-sensors",children:"Types of Sensors"}),"\n",(0,t.jsx)(n.h3,{id:"1-vision-sensors",children:"1. Vision Sensors"}),"\n",(0,t.jsx)(n.h4,{id:"cameras",children:"Cameras"}),"\n",(0,t.jsx)(n.p,{children:"The most common vision sensors in robotics:"}),"\n",(0,t.jsx)(s,{title:"Camera Types in Robotics",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:'graph LR\n    subgraph "2D Cameras"\n        A[RGB Camera]\n        B[Monochrome Camera]\n        C[Thermal Camera]\n    end\n\n    subgraph "3D Cameras"\n        D[Stereoscopic]\n        E[Time of Flight]\n        F[Structured Light]\n    end\n\n    subgraph "Specialized"\n        G[Event Camera]\n        H[Hyperspectral]\n        I[360\xb0 Camera]\n    end\n'})})}),"\n",(0,t.jsx)(o.A,{title:"OpenCV Camera Processing Example",language:"python",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\nclass CameraProcessor:\n    def __init__(self, camera_id=0):\n        self.camera = cv2.VideoCapture(camera_id)\n        self.frame_count = 0\n\n    def capture_frame(self):\n        \"\"\"Capture and process a single frame\"\"\"\n        ret, frame = self.camera.read()\n        if not ret:\n            return None\n\n        # Convert to different color spaces\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n        # Apply filters\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n        edges = cv2.Canny(blurred, 50, 150)\n\n        return {\n            'original': frame,\n            'gray': gray,\n            'hsv': hsv,\n            'edges': edges\n        }\n\n    def detect_objects(self, frame):\n        \"\"\"Detect objects using color thresholding\"\"\"\n        hsv = frame['hsv']\n\n        # Example: Detect red objects\n        lower_red = np.array([0, 100, 100])\n        upper_red = np.array([10, 255, 255])\n\n        mask = cv2.inRange(hsv, lower_red, upper_red)\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Draw bounding boxes\n        result = frame['original'].copy()\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:\n                x, y, w, h = cv2.boundingRect(contour)\n                cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n        return result, len(contours)\n"})})}),"\n",(0,t.jsx)(n.h3,{id:"2-depth-sensors",children:"2. Depth Sensors"}),"\n",(0,t.jsx)(n.h4,{id:"lidar-light-detection-and-ranging",children:"LiDAR (Light Detection and Ranging)"}),"\n",(0,t.jsx)(n.p,{children:"LiDAR sensors use laser beams to measure distance, creating 3D point clouds of the environment."}),"\n",(0,t.jsx)(s,{title:"LiDAR Scanning Pattern",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "LiDAR System"\n        A[Laser Emitter] --\x3e B[Rotating Mirror]\n        B --\x3e C[Environment]\n        C --\x3e D[Photodetector]\n        D --\x3e E[Distance Calculation]\n        E --\x3e F[3D Point Cloud]\n    end\n\n    G[360\xb0 View] --\x3e F\n    H[Obstacle Detection] --\x3e F\n    I[Mapping] --\x3e F\n'})})}),"\n",(0,t.jsx)(o.A,{title:"LiDAR Data Processing",language:"python",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nclass LidarProcessor:\n    def __init__(self):\n        self.max_range = 100.0  # meters\n        self.angle_resolution = 0.25  # degrees\n\n    def process_scan(self, raw_data):\n        """Process raw LiDAR scan data into point cloud"""\n        points = []\n\n        for i, distance in enumerate(raw_data):\n            if 0.1 < distance < self.max_range:  # Filter valid ranges\n                angle = np.radians(i * self.angle_resolution)\n\n                # Convert polar to Cartesian coordinates\n                x = distance * np.cos(angle)\n                y = distance * np.sin(angle)\n                z = 0  # Assume 2D LiDAR\n\n                points.append([x, y, z])\n\n        return np.array(points)\n\n    def segment_obstacles(self, point_cloud, threshold=0.5):\n        """Segment point cloud into obstacles using clustering"""\n        from sklearn.cluster import DBSCAN\n\n        # Perform DBSCAN clustering\n        clustering = DBSCAN(eps=threshold, min_samples=5).fit(point_cloud)\n        labels = clustering.labels_\n\n        # Group points by cluster\n        obstacles = {}\n        for i, label in enumerate(labels):\n            if label != -1:  # -1 represents noise\n                if label not in obstacles:\n                    obstacles[label] = []\n                obstacles[label].append(point_cloud[i])\n\n        return obstacles\n\n    def create_occupancy_grid(self, point_cloud, grid_size=200, resolution=0.5):\n        """Create 2D occupancy grid from point cloud"""\n        grid = np.zeros((grid_size, grid_size))\n\n        for point in point_cloud:\n            # Convert point to grid coordinates\n            x_idx = int((point[0] + grid_size * resolution / 2) / resolution)\n            y_idx = int((point[1] + grid_size * resolution / 2) / resolution)\n\n            if 0 <= x_idx < grid_size and 0 <= y_idx < grid_size:\n                grid[y_idx, x_idx] = 1  # Mark as occupied\n\n        return grid\n'})})}),"\n",(0,t.jsx)(n.h3,{id:"3-proximity-sensors",children:"3. Proximity Sensors"}),"\n",(0,t.jsx)(n.h4,{id:"ultrasonic-sensors",children:"Ultrasonic Sensors"}),"\n",(0,t.jsx)(n.p,{children:"Using sound waves to measure distance:"}),"\n",(0,t.jsx)(o.A,{title:"Ultrasonic Sensor Simulation",language:"python",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import time\nimport random\n\nclass UltrasonicSensor:\n    def __init__(self, max_distance=400, min_distance=2):\n        """Initialize ultrasonic sensor\n        max_distance: maximum detection distance in cm\n        min_distance: minimum detection distance in cm\n        """\n        self.max_distance = max_distance\n        self.min_distance = min_distance\n        self.speed_of_sound = 34300  # cm/s in air at 20\xb0C\n\n    def measure_distance(self):\n        """Simulate distance measurement"""\n        # Simulate echo time calculation\n        distance = random.uniform(self.min_distance, self.max_distance)\n\n        # Calculate echo time (microseconds)\n        echo_time = (2 * distance / self.speed_of_sound) * 1e6\n\n        return distance, echo_time\n\n    def detect_obstacles(self, angle, distance_threshold=50):\n        """Detect obstacles at a specific angle"""\n        distance, _ = self.measure_distance()\n\n        if distance < distance_threshold:\n            return {\n                \'obstacle_detected\': True,\n                \'angle\': angle,\n                \'distance\': distance,\n                \'confidence\': min(1.0, (distance_threshold - distance) / distance_threshold)\n            }\n        else:\n            return {\n                \'obstacle_detected\': False,\n                \'angle\': angle,\n                \'distance\': distance,\n                \'confidence\': 0.0\n            }\n\n# Example: Multiple ultrasonic sensors for 360\xb0 coverage\nclass MultiUltrasonicArray:\n    def __init__(self, num_sensors=8):\n        self.sensors = [UltrasonicSensor() for _ in range(num_sensors)]\n        self.angles = [i * (360 / num_sensors) for i in range(num_sensors)]\n\n    def scan_360(self):\n        """Perform a 360\xb0 scan"""\n        results = []\n\n        for sensor, angle in zip(self.sensors, self.angles):\n            result = sensor.detect_obstacles(angle)\n            results.append(result)\n\n        return results\n\n# Create and test sensor array\nsensor_array = MultiUltrasonicArray(8)\nscan_results = sensor_array.scan_360()\n\n# Find closest obstacle\nobstacles = [r for r in scan_results if r[\'obstacle_detected\']]\nif obstacles:\n    closest = min(obstacles, key=lambda x: x[\'distance\'])\n    print(f"Closest obstacle at {closest[\'angle\']}\xb0, {closest[\'distance\']}cm")\n'})})}),"\n",(0,t.jsx)(n.h3,{id:"4-inertial-sensors",children:"4. Inertial Sensors"}),"\n",(0,t.jsx)(n.h4,{id:"imu-inertial-measurement-unit",children:"IMU (Inertial Measurement Unit)"}),"\n",(0,t.jsx)(n.p,{children:"Combines accelerometer and gyroscope to measure motion and orientation."}),"\n",(0,t.jsx)(s,{title:"IMU Components",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "IMU"\n        A[Accelerometer] --\x3e C[Sensor Fusion]\n        B[Gyroscope] --\x3e C\n        D[Magnetometer] --\x3e C\n        C --\x3e E[Orientation]\n        C --\x3e F[Velocity]\n        C --\x3e G[Position]\n    end\n'})})}),"\n",(0,t.jsx)(o.A,{title:"IMU Data Processing",language:"python",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy.integrate import cumtrapz\nimport matplotlib.pyplot as plt\n\nclass IMUProcessor:\n    def __init__(self, sample_rate=100):\n        self.sample_rate = sample_rate  # Hz\n        self.dt = 1.0 / sample_rate\n\n        # State variables\n        self.orientation = np.array([0, 0, 0])  # roll, pitch, yaw in radians\n        self.velocity = np.array([0, 0, 0])\n        self.position = np.array([0, 0, 0])\n\n        # Calibration parameters\n        self.accel_bias = np.array([0.0, 0.0, 0.0])\n        self.gyro_bias = np.array([0.0, 0.0, 0.0])\n\n    def calibrate(self, accel_data, gyro_data, duration=5.0):\n        """Calibrate IMU by computing bias from stationary measurements"""\n        samples = int(duration * self.sample_rate)\n\n        self.accel_bias = np.mean(accel_data[:samples], axis=0)\n        self.gyro_bias = np.mean(gyro_data[:samples], axis=0)\n\n        # Account for gravity in Z-axis accelerometer bias\n        self.accel_bias[2] -= 9.81\n\n        return self.accel_bias, self.gyro_bias\n\n    def process_accelerometer(self, accel_data):\n        """Process accelerometer data to get linear acceleration"""\n        # Remove bias\n        accel_corrected = accel_data - self.accel_bias\n\n        # Integrate to get velocity\n        self.velocity += cumtrapz(accel_corrected, dx=self.dt, initial=0)\n\n        # Integrate to get position\n        self.position += cumtrapz(self.velocity, dx=self.dt, initial=0)\n\n        return self.velocity, self.position\n\n    def process_gyroscope(self, gyro_data):\n        """Process gyroscope data to update orientation"""\n        # Remove bias\n        gyro_corrected = gyro_data - self.gyro_bias\n\n        # Integrate angular velocity to get orientation\n        self.orientation += cumtrapz(gyro_corrected, dx=self.dt, initial=0)\n\n        # Normalize angles to [-\u03c0, \u03c0]\n        self.orientation = np.arctan2(np.sin(self.orientation), np.cos(self.orientation))\n\n        return self.orientation\n\n    def sensor_fusion_kalman(self, accel_data, gyro_data):\n        """\n        Simple Kalman filter for sensor fusion\n        Combines accelerometer and gyroscope data\n        """\n        # State: [position, velocity, orientation]\n        x = np.array([self.position, self.velocity, self.orientation])\n\n        # Process noise covariance\n        Q = np.eye(9) * 0.01\n\n        # Measurement noise covariance\n        R = np.eye(9) * 0.1\n\n        # Simple Kalman filter implementation\n        # Prediction step\n        F = np.eye(9)  # State transition matrix\n        x_pred = F @ x\n\n        # Update step with accelerometer\n        H_accel = np.eye(9)  # Measurement matrix\n        z_accel = np.array([self.position, self.velocity, self.orientation])\n\n        # Kalman gain\n        S = H_accel @ Q @ H_accel.T + R\n        K = Q @ H_accel.T @ np.linalg.inv(S)\n\n        # Update state\n        x = x_pred + K @ (z_accel - H_accel @ x_pred)\n\n        # Update state variables\n        self.position = x[:3]\n        self.velocity = x[3:6]\n        self.orientation = x[6:9]\n\n        return x\n\n# Example usage\nimu = IMUProcessor(sample_rate=100)\n\n# Simulate IMU data\nt = np.arange(0, 10, 1/100)\naccel_data = np.random.randn(1000, 3) * 0.1 + np.array([0, 0, 9.81])\ngyro_data = np.random.randn(1000, 3) * 0.01\n\n# Process data\nfor i in range(len(accel_data)):\n    accel = accel_data[i]\n    gyro = gyro_data[i]\n\n    velocity, position = imu.process_accelerometer(accel)\n    orientation = imu.process_gyroscope(gyro)\n\n    if i % 100 == 0:  # Log every 1 second\n        print(f"Time: {i/100:.1f}s, Position: {position}")\n'})})}),"\n",(0,t.jsx)(n.h3,{id:"5-specialized-sensors",children:"5. Specialized Sensors"}),"\n",(0,t.jsx)(n.h4,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,t.jsx)(n.p,{children:"Measure contact forces and torques for manipulation tasks."}),"\n",(0,t.jsx)(o.A,{title:"Force Sensor Integration",language:"python",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ForceSensor:\n    def __init__(self, max_force=50, max_torque=5):\n        self.max_force = max_force  # Newtons\n        self.max_torque = max_torque  # N\u22c5m\n        self.calibration_matrix = np.eye(6)  # 6-DOF calibration\n\n    def read_forces(self):\n        \"\"\"Read force/torque data\"\"\"\n        # Simulate force sensor reading\n        fx = np.random.uniform(-self.max_force, self.max_force)\n        fy = np.random.uniform(-self.max_force, self.max_force)\n        fz = np.random.uniform(0, self.max_force)  # Positive Z (pushing)\n        tx = np.random.uniform(-self.max_torque, self.max_torque)\n        ty = np.random.uniform(-self.max_torque, self.max_torque)\n        tz = np.random.uniform(-self.max_torque, self.max_torque)\n\n        raw_reading = np.array([fx, fy, fz, tx, ty, tz])\n\n        # Apply calibration\n        calibrated_reading = self.calibration_matrix @ raw_reading\n\n        return {\n            'force': calibrated_reading[:3],\n            'torque': calibrated_reading[3:6],\n            'magnitude': {\n                'force': np.linalg.norm(calibrated_reading[:3]),\n                'torque': np.linalg.norm(calibrated_reading[3:6])\n            }\n        }\n\nclass GripperWithForceFeedback:\n    def __init__(self):\n        self.force_sensor = ForceSensor()\n        self.gripper_open = True\n        self.grasp_force = 10.0  # Newtons\n        self.current_force = 0.0\n\n    def grasp_object(self, target_force=None):\n        \"\"\"Grasp an object with force feedback\"\"\"\n        if target_force:\n            self.grasp_force = target_force\n\n        # Close gripper\n        self.gripper_open = False\n\n        # Monitor force while closing\n        while self.current_force < self.grasp_force:\n            force_data = self.force_sensor.read_forces()\n            self.current_force = force_data['magnitude']['force']\n\n            # Check if object is slipping\n            if self.current_force > self.grasp_force * 1.5:\n                print(\"Warning: Object might be slipping!\")\n                break\n\n        return {\n            'grasped': True,\n            'applied_force': self.current_force,\n            'stable': self.current_force >= self.grasp_force * 0.9\n        }\n\n# Test the force feedback system\ngripper = GripperWithForceFeedback()\nresult = gripper.grasp_object(target_force=15.0)\nprint(f\"Grasp result: {result}\")\n"})})}),"\n",(0,t.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,t.jsx)(n.h3,{id:"why-fuse-sensors",children:"Why Fuse Sensors?"}),"\n",(0,t.jsx)(n.p,{children:"Different sensors have different strengths and weaknesses:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Sensor Type"}),(0,t.jsx)(n.th,{children:"Strengths"}),(0,t.jsx)(n.th,{children:"Weaknesses"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Camera"}),(0,t.jsx)(n.td,{children:"Rich visual info"}),(0,t.jsx)(n.td,{children:"Sensitive to lighting"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"LiDAR"}),(0,t.jsx)(n.td,{children:"Accurate depth"}),(0,t.jsx)(n.td,{children:"Expensive, sparse data"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"IMU"}),(0,t.jsx)(n.td,{children:"High frequency"}),(0,t.jsx)(n.td,{children:"Drift over time"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"GPS"}),(0,t.jsx)(n.td,{children:"Global position"}),(0,t.jsx)(n.td,{children:"Not available indoors"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"fusion-techniques",children:"Fusion Techniques"}),"\n",(0,t.jsx)(o.A,{title:"Kalman Filter for Sensor Fusion",language:"python",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom filterpy.kalman import KalmanFilter\n\nclass MultiSensorFusion:\n    def __init__(self):\n        # State vector: [x, y, z, vx, vy, vz, ax, ay, az]\n        self.kf = KalmanFilter(dim_x=9, dim_z=9)\n\n        # State transition matrix\n        self.kf.F = np.array([\n            [1, 0, 0, 1, 0, 0, 0.5, 0, 0],  # x = x + vx*dt + 0.5*ax*dt^2\n            [0, 1, 0, 0, 1, 0, 0, 0.5, 0],  # y = y + vy*dt + 0.5*ay*dt^2\n            [0, 0, 1, 0, 0, 1, 0, 0, 0.5],  # z = z + vz*dt + 0.5*az*dt^2\n            [0, 0, 0, 1, 0, 0, 1, 0, 0],      # vx = vx + ax*dt\n            [0, 0, 0, 0, 1, 0, 0, 1, 0],      # vy = vy + ay*dt\n            [0, 0, 0, 0, 0, 1, 0, 0, 1],      # vz = vz + az*dt\n            [0, 0, 0, 0, 0, 0, 1, 0, 0],      # ax = ax (random walk)\n            [0, 0, 0, 0, 0, 0, 0, 1, 0],      # ay = ay (random walk)\n            [0, 0, 0, 0, 0, 0, 0, 0, 1]       # az = az (random walk)\n        ])\n\n        # Measurement matrix\n        self.kf.H = np.eye(9)\n\n        # Process noise\n        self.kf.Q *= 0.01\n\n        # Measurement noise\n        self.kf.R *= 0.1\n\n        # Initial state covariance\n        self.kf.P *= 1000\n\n    def update_with_vision(self, position, confidence=0.8):\n        """Update Kalman filter with vision data"""\n        # Adjust measurement noise based on confidence\n        R_vision = self.kf.R / confidence\n        self.kf.R = R_vision\n\n        z = np.array([position[0], position[1], position[2], 0, 0, 0, 0, 0, 0])\n        self.kf.predict()\n        self.kf.update(z)\n\n        return self.kf.x[:3]\n\n    def update_with_imu(self, acceleration, confidence=0.9):\n        """Update Kalman filter with IMU data"""\n        # Adjust measurement noise\n        R_imu = self.kf.R / confidence\n        self.kf.R = R_imu\n\n        z = np.array([0, 0, 0, 0, 0, 0, acceleration[0], acceleration[1], acceleration[2]])\n        self.kf.predict()\n        self.kf.update(z)\n\n        return self.kf.x\n\n    def update_with_lidar(self, position, confidence=0.95):\n        """Update Kalman filter with LiDAR data"""\n        R_lidar = self.kf.R / confidence\n        self.kf.R = R_lidar\n\n        z = np.array([position[0], position[1], position[2], 0, 0, 0, 0, 0, 0])\n        self.kf.predict()\n        self.kf.update(z)\n\n        return self.kf.x[:3]\n\n# Example usage\nfusion = MultiSensorFusion()\n\n# Simulate sensor updates over time\nfor t in range(10):\n    # Vision update\n    vision_pos = np.array([t + np.random.normal(0, 0.1),\n                           2 + np.random.normal(0, 0.1),\n                           0])\n    fusion.update_with_vision(vision_pos)\n\n    # IMU update\n    imu_accel = np.array([0.1, 0, -9.81])  # Small forward acceleration and gravity\n    fusion.update_with_imu(imu_accel)\n\n    # LiDAR update (every 3 steps)\n    if t % 3 == 0:\n        lidar_pos = np.array([t + np.random.normal(0, 0.05),\n                              2 + np.random.normal(0, 0.05),\n                              np.random.normal(0, 0.05)])\n        fusion.update_with_lidar(lidar_pos)\n\n    # Get fused estimate\n    estimated_state = fusion.kf.x\n    print(f"Time {t}: Position = ({estimated_state[0]:.2f}, {estimated_state[1]:.2f}, {estimated_state[2]:.2f})")\n'})})}),"\n",(0,t.jsx)(n.h2,{id:"perception-pipeline",children:"Perception Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"end-to-end-perception-system",children:"End-to-End Perception System"}),"\n",(0,t.jsx)(s,{title:"Robot Perception Pipeline",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"flowchart TD\n    A[Raw Sensor Data] --\x3e B[Preprocessing]\n    B --\x3e C[Feature Extraction]\n    C --\x3e D[Sensor Fusion]\n    D --\x3e E[Object Detection]\n    E --\x3e F[Scene Understanding]\n    F --\x3e G[Decision Making]\n\n    B --\x3e B1[Noise Filtering]\n    B --\x3e B2[Calibration]\n    B --\x3e B3[Normalization]\n\n    C --\x3e C1[Edge Detection]\n    C --\x3e C2[Feature Points]\n    C --\x3e C3[Descriptors]\n\n    D --\x3e D1[Kalman Filter]\n    D --\x3e D2[Particle Filter]\n    D --\x3e D3[Bayesian Fusion]\n\n    E --\x3e E1[YOLO / SSD]\n    E --\x3e E2[R-CNN]\n    E --\x3e E3[PointNet]\n"})})}),"\n",(0,t.jsx)(n.h2,{id:"lab-exercise-building-a-simple-perception-system",children:"Lab Exercise: Building a Simple Perception System"}),"\n",(0,t.jsxs)("div",{className:"lab-exercise",children:[(0,t.jsx)(n.h3,{id:"objective",children:"Objective"}),(0,t.jsx)(n.p,{children:"Create a multi-sensor perception system that can detect and track objects."}),(0,t.jsx)(n.h3,{id:"setup",children:"Setup"}),(0,t.jsx)(n.p,{children:"We'll build a simplified perception system using simulated sensor data."}),(0,t.jsx)(n.h3,{id:"implementation",children:"Implementation"}),(0,t.jsx)(o.A,{language:"python",editable:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import euclidean\n\nclass PerceptionSystem:\n    def __init__(self):\n        self.objects = []\n        self.object_tracker = {}\n        self.next_id = 0\n\n        # Sensor noise parameters\n        self.vision_noise = 0.1\n        self.lidar_noise = 0.05\n\n    def detect_objects_vision(self, camera_data):\n        \"\"\"Simulate object detection from camera\"\"\"\n        objects = []\n\n        # Simulate detecting 3 objects in the scene\n        true_objects = [\n            {'id': 1, 'pos': [2, 3], 'size': 1.0, 'type': 'cube'},\n            {'id': 2, 'pos': [5, 1], 'size': 1.5, 'type': 'sphere'},\n            {'id': 3, 'pos': [3, 5], 'size': 0.8, 'type': 'cylinder'}\n        ]\n\n        for obj in true_objects:\n            # Add Gaussian noise to measurements\n            measured_pos = [\n                obj['pos'][0] + np.random.normal(0, self.vision_noise),\n                obj['pos'][1] + np.random.normal(0, self.vision_noise)\n            ]\n\n            objects.append({\n                'sensor': 'vision',\n                'position': np.array(measured_pos),\n                'size': obj['size'],\n                'type': obj['type'],\n                'confidence': 0.9\n            })\n\n        return objects\n\n    def detect_objects_lidar(self, lidar_data):\n        \"\"\"Simulate object detection from LiDAR\"\"\"\n        objects = []\n\n        # Simulate detecting 2 objects (less resolution than vision)\n        true_objects = [\n            {'id': 1, 'pos': [2.05, 2.95], 'type': 'cube'},\n            {'id': 2, 'pos': [4.95, 1.02], 'type': 'sphere'}\n        ]\n\n        for obj in true_objects:\n            # Add smaller noise to LiDAR measurements\n            measured_pos = [\n                obj['pos'][0] + np.random.normal(0, self.lidar_noise),\n                obj['pos'][1] + np.random.normal(0, self.lidar_noise)\n            ]\n\n            objects.append({\n                'sensor': 'lidar',\n                'position': np.array(measured_pos),\n                'type': obj['type'],\n                'confidence': 0.95\n            })\n\n        return objects\n\n    def fuse_detections(self, vision_objects, lidar_objects):\n        \"\"\"Fuse detections from multiple sensors\"\"\"\n        fused_objects = []\n\n        # Combine all detections\n        all_detections = vision_objects + lidar_objects\n\n        # Group by type and position\n        for detection in all_detections:\n            # Check if this object matches any existing fused object\n            matched = False\n\n            for fused in fused_objects:\n                # Check if same type and close position\n                if (fused['type'] == detection['type'] and\n                    euclidean(fused['position'], detection['position']) < 0.5):\n\n                    # Weighted average of positions\n                    weight_vision = detection['confidence'] if detection['sensor'] == 'vision' else 0\n                    weight_lidar = detection['confidence'] if detection['sensor'] == 'lidar' else 0\n\n                    total_weight = fused['vision_confidence'] + fused['lidar_confidence'] + weight_vision + weight_lidar\n\n                    fused['position'] = (\n                        fused['position'] * (fused['vision_confidence'] + fused['lidar_confidence']) +\n                        detection['position'] * (weight_vision + weight_lidar)\n                    ) / total_weight\n\n                    fused['vision_confidence'] += weight_vision\n                    fused['lidar_confidence'] += weight_lidar\n                    fused['total_confidence'] = fused['vision_confidence'] + fused['lidar_confidence']\n\n                    matched = True\n                    break\n\n            if not matched:\n                # Create new fused object\n                fused_objects.append({\n                    'type': detection['type'],\n                    'position': detection['position'].copy(),\n                    'vision_confidence': detection['confidence'] if detection['sensor'] == 'vision' else 0,\n                    'lidar_confidence': detection['confidence'] if detection['sensor'] == 'lidar' else 0,\n                    'total_confidence': detection['confidence']\n                })\n\n        return fused_objects\n\n    def track_objects(self, current_objects, prev_objects):\n        \"\"\"Track objects across frames\"\"\"\n        if not prev_objects:\n            # First frame - assign new IDs\n            for obj in current_objects:\n                obj['id'] = self.next_id\n                self.next_id += 1\n                self.object_tracker[obj['id']] = obj\n        else:\n            # Match current objects with previous ones\n            matched_prev = set()\n\n            for curr_obj in current_objects:\n                best_match = None\n                best_distance = float('inf')\n                best_id = None\n\n                for prev_id, prev_obj in prev_objects.items():\n                    if prev_id in matched_prev:\n                        continue\n\n                    # Check if same type and close position\n                    if (curr_obj['type'] == prev_obj['type']):\n                        distance = euclidean(curr_obj['position'], prev_obj['position'])\n\n                        if distance < 1.0 and distance < best_distance:\n                            best_match = prev_obj\n                            best_distance = distance\n                            best_id = prev_id\n\n                if best_match:\n                    # Update existing track\n                    curr_obj['id'] = best_id\n                    self.object_tracker[best_id] = curr_obj\n                    matched_prev.add(best_id)\n                else:\n                    # New object detected\n                    curr_obj['id'] = self.next_id\n                    self.next_id += 1\n                    self.object_tracker[curr_obj['id']] = curr_obj\n\n            # Remove objects that were not detected (they left the scene)\n            for prev_id in list(prev_objects.keys()):\n                if prev_id not in matched_prev:\n                    del self.object_tracker[prev_id]\n\n        return list(self.object_tracker.values())\n\n# Create and test perception system\nperception = PerceptionSystem()\n\n# Simulate multiple frames\nprint(\"=== Multi-Sensor Perception System ===\\n\")\n\nframe_count = 0\ntracked_objects = None\n\nwhile frame_count < 5:\n    print(f\"\\n--- Frame {frame_count + 1} ---\")\n\n    # Get detections from sensors\n    vision_objects = perception.detect_objects_vision(None)\n    lidar_objects = perception.detect_objects_lidar(None)\n\n    print(f\"Vision detected {len(vision_objects)} objects\")\n    print(f\"LiDAR detected {len(lidar_objects)} objects\")\n\n    # Fuse detections\n    fused_objects = perception.fuse_detections(vision_objects, lidar_objects)\n    print(f\"Fused detections: {len(fused_objects)} objects\")\n\n    # Track objects across frames\n    tracked_objects = perception.track_objects(fused_objects, tracked_objects)\n\n    print(\"\\nTracked objects:\")\n    for obj in tracked_objects:\n        print(f\"  ID {obj['id']}: {obj['type']} at ({obj['position'][0]:.2f}, {obj['position'][1]:.2f}) \"\n              f\"confidence: {obj['total_confidence']:.2f}\")\n\n    frame_count += 1\n"})})}),(0,t.jsx)(n.h3,{id:"expected-output",children:"Expected Output"}),(0,t.jsx)(n.p,{children:"The system should demonstrate:"}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Multi-sensor object detection"}),"\n",(0,t.jsx)(n.li,{children:"Sensor fusion with confidence weighting"}),"\n",(0,t.jsx)(n.li,{children:"Object tracking across frames"}),"\n",(0,t.jsx)(n.li,{children:"Handling of new and disappearing objects"}),"\n"]}),(0,t.jsx)(n.h3,{id:"extension-exercises",children:"Extension Exercises"}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Add a third sensor (e.g., thermal camera)"}),"\n",(0,t.jsx)(n.li,{children:"Implement Kalman filtering for smoother tracking"}),"\n",(0,t.jsx)(n.li,{children:"Add object classification using type-specific features"}),"\n",(0,t.jsx)(n.li,{children:"Visualize the tracking results"}),"\n"]})]}),"\n",(0,t.jsx)(n.h2,{id:"challenges-in-robot-perception",children:"Challenges in Robot Perception"}),"\n",(0,t.jsx)(n.h3,{id:"1-environmental-variability",children:"1. Environmental Variability"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Lighting changes affect vision sensors"}),"\n",(0,t.jsx)(n.li,{children:"Weather conditions impact outdoor perception"}),"\n",(0,t.jsx)(n.li,{children:"Dynamic environments require continuous adaptation"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-sensor-limitations",children:"2. Sensor Limitations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Each sensor type has inherent limitations"}),"\n",(0,t.jsx)(n.li,{children:"No single sensor provides complete information"}),"\n",(0,t.jsx)(n.li,{children:"Fusion must handle different data types and rates"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-real-time-constraints",children:"3. Real-Time Constraints"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Perception must happen in milliseconds"}),"\n",(0,t.jsx)(n.li,{children:"Limited computational resources on robots"}),"\n",(0,t.jsx)(n.li,{children:"Trade-off between accuracy and speed"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-ambiguity-and-uncertainty",children:"4. Ambiguity and Uncertainty"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Perceptual aliasing (different objects look similar)"}),"\n",(0,t.jsx)(n.li,{children:"Noise and measurement errors"}),"\n",(0,t.jsx)(n.li,{children:"Partial observations"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"5-scale-and-range",children:"5. Scale and Range"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robots need to perceive at multiple scales"}),"\n",(0,t.jsx)(n.li,{children:"Far objects require different sensing than near objects"}),"\n",(0,t.jsx)(n.li,{children:"Different environments need different sensing strategies"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensors are the robot's senses"})," - They enable perception of the physical world"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Different sensors provide complementary information"})," - Vision gives rich detail, LiDAR gives accurate depth"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor fusion creates robust perception"})," - Combining sensors overcomes individual limitations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Processing is essential"})," - Raw sensor data must be processed into useful information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tracking enables understanding"})," - Maintaining object identity over time is crucial"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Perception systems are the foundation of Physical AI, enabling robots to understand and interact with their environment. By combining multiple sensors and sophisticated processing algorithms, robots can build rich models of the world that support intelligent decision-making."}),"\n",(0,t.jsxs)(n.p,{children:["In the next lesson, we'll explore the ",(0,t.jsx)(n.strong,{children:"weekly learning plan"})," and how to structure your 13-week journey through Physical AI."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"lesson-3",children:"Next: Weekly Learning Plan (Weeks 1-2) \u2192"})}),"\n",(0,t.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,t.jsx)(i,{quizId:"sensors-perception",questions:[{id:"q1",type:"multiple-choice",question:"Which sensor type provides the most accurate depth information?",options:["RGB Camera","LiDAR","IMU","Ultrasonic"],correct:1,explanation:"LiDAR provides the most accurate depth information by directly measuring distance with laser beams."},{id:"q2",type:"multiple-choice",question:"What is the main advantage of sensor fusion?",options:["Reduces cost","Increases processing speed","Overcomes individual sensor limitations","Requires fewer sensors"],correct:2,explanation:"Sensor fusion combines strengths of different sensors to overcome their individual limitations, creating more robust perception."},{id:"q3",type:"true-false",question:"IMU sensors can provide absolute position information without drift.",correct:!1,explanation:"IMUs suffer from integration drift over time and cannot provide absolute position without external reference."}]})]})}function f(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}function u(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var i=s(6540);const t={},r=i.createContext(t);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);