"use strict";(globalThis.webpackChunkphysical_ai_robotics_textbook=globalThis.webpackChunkphysical_ai_robotics_textbook||[]).push([[8930],{7548:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>f,frontMatter:()=>l,metadata:()=>s,toc:()=>u});const s=JSON.parse('{"id":"chapter-4/lesson-4","title":"Lesson 4.4: Capstone Project - Autonomous Humanoid Robot","description":"Building a complete autonomous humanoid robot system integrating all course concepts","source":"@site/docs/chapter-4/lesson-4.mdx","sourceDirName":"chapter-4","slug":"/chapter-4/lesson-4","permalink":"/docs/chapter-4/lesson-4","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai/physical-ai-robotics-textbook/tree/main/docs/chapter-4/lesson-4.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Lesson 4.4: Capstone Project - Autonomous Humanoid Robot","description":"Building a complete autonomous humanoid robot system integrating all course concepts","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.3: Balance and Manipulation","permalink":"/docs/chapter-4/lesson-3"}}');var a=t(4848),o=t(8453),i=t(101),r=t(2948);const l={title:"Lesson 4.4: Capstone Project - Autonomous Humanoid Robot",description:"Building a complete autonomous humanoid robot system integrating all course concepts",sidebar_position:4},c="Lesson 4.4: Capstone Project - Autonomous Humanoid Robot",p={},u=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"System Components",id:"system-components",level:2},{value:"1. Autonomous Navigation System",id:"1-autonomous-navigation-system",level:3},{value:"2. Task Planning and Execution",id:"2-task-planning-and-execution",level:3},{value:"3. Integrated Control System",id:"3-integrated-control-system",level:3},{value:"Lab Exercise: Complete System Integration",id:"lab-exercise-complete-system-integration",level:2},{value:"Objective",id:"objective",level:3},{value:"Setup",id:"setup",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Expected Outcomes",id:"expected-outcomes",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:3},{value:"Future Enhancements",id:"future-enhancements",level:2},{value:"1. Advanced AI Integration",id:"1-advanced-ai-integration",level:3},{value:"2. Enhanced Sensing",id:"2-enhanced-sensing",level:3},{value:"3. Social Intelligence",id:"3-social-intelligence",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Summary",id:"summary",level:2},{value:"Final Quiz",id:"final-quiz",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{DiagramComponent:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("DiagramComponent",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lesson-44-capstone-project---autonomous-humanoid-robot",children:"Lesson 4.4: Capstone Project - Autonomous Humanoid Robot"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)("div",{className:"learning-objectives",children:[(0,a.jsx)(n.p,{children:"After completing this capstone project, you will have:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Integrated all course concepts into a complete humanoid robot system"}),"\n",(0,a.jsx)(n.li,{children:"Implemented perception-action loops for autonomous operation"}),"\n",(0,a.jsx)(n.li,{children:"Created a robot that can navigate, manipulate, and interact naturally"}),"\n",(0,a.jsx)(n.li,{children:"Developed task planning and execution capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Built a system that learns and adapts from experience"}),"\n"]})]}),"\n",(0,a.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,a.jsx)(n.p,{children:"This capstone project challenges you to build a complete autonomous humanoid robot system that integrates:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Physical AI principles"})," from Chapter 1"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS 2 and simulation"})," from Chapter 2"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"NVIDIA Isaac and VLA systems"})," from Chapter 3"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Humanoid kinematics, locomotion, and manipulation"})," from Chapter 4"]}),"\n"]}),"\n",(0,a.jsx)(t,{title:"Complete System Architecture",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "Perception Layer"\n        A[Vision System<br/>Object Detection]\n        B[Audio System<br/>Speech Recognition]\n        C[Tactile Sensors<br/>Force Feedback]\n        D[IMU<br/>Balance/Posture]\n    end\n\n    subgraph "Cognitive Layer"\n        E[VLA System<br/>Understanding]\n        F[Task Planner<br/>Sequencing]\n        G[Motion Planner<br/>Trajectories]\n        H[Learning Module<br/>Adaptation]\n    end\n\n    subgraph "Control Layer"\n        I[Biped Locomotion<br/>Walking/Balance]\n        J[Manipulation<br/>Grasping/Placement]\n        K[Whole-body<br/>Coordination]\n        L[Reactive Control<br/>Perturbation Handling]\n    end\n\n    subgraph "Execution Layer"\n        M[Joint Controllers<br/>Motors/Actuators]\n        N[End-effectors<br/>Hands/Grippers]\n        O[Communication<br/>ROS 2 Network]\n    end\n\n    A --\x3e E\n    B --\x3e E\n    C --\x3e E\n    D --\x3e E\n    E --\x3e F\n    F --\x3e G\n    G --\x3e I\n    G --\x3e J\n    I --\x3e K\n    J --\x3e K\n    H --\x3e G\n    K --\x3e M\n    K --\x3e N\n    M --\x3e O\n    N --\x3e O\n'})})}),"\n",(0,a.jsx)(n.h2,{id:"system-components",children:"System Components"}),"\n",(0,a.jsx)(n.h3,{id:"1-autonomous-navigation-system",children:"1. Autonomous Navigation System"}),"\n",(0,a.jsx)(r.A,{title:"Integrated Navigation System",language:"python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import LaserScan, Image\nfrom nav_msgs.msg import OccupancyGrid, Path\nfrom std_msgs.msg import String\nimport torch\nfrom typing import Dict, List, Tuple, Optional\n\nclass AutonomousNavigator(Node):\n    \"\"\"Integrated navigation system for humanoid robot\"\"\"\n\n    def __init__(self):\n        super().__init__('autonomous_navigator')\n\n        # Navigation parameters\n        self.current_pose = np.array([0.0, 0.0, 0.0])\n        self.current_goal = None\n        self.current_path = []\n        self.navigating = False\n\n        # Perception systems\n        self.vision_system = VisionLanguageActionSystem()\n        self.path_planner = GlobalPathPlanner()\n        self.local_planner = LocalPathPlanner()\n\n        # Subscriptions\n        self.pose_sub = self.create_subscription(\n            PoseStamped, '/robot/pose', self.pose_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/robot/scan', self.scan_callback, 10\n        )\n        self.image_sub = self.create_subscription(\n            Image, '/robot/camera/image_raw', self.image_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, '/navigation/command', self.command_callback, 10\n        )\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(\n            Twist, '/robot/cmd_vel', 10\n        )\n        self.path_pub = self.create_publisher(\n            Path, '/navigation/path', 10\n        )\n        self.status_pub = self.create_publisher(\n            String, '/navigation/status', 10\n        )\n\n        # Navigation state\n        self.obstacles = []\n        self.map_resolution = 0.05\n        self.map_size = (200, 200)  # 10m x 10m\n\n        # Timer for navigation loop\n        self.navigation_timer = self.create_timer(\n            0.1, self.navigation_loop\n        )\n\n        self.get_logger().info('Autonomous navigator initialized')\n\n    def navigate_to_goal(self, goal_description: str, goal_pose: Optional[np.ndarray] = None):\n        \"\"\"\n        Navigate to goal using natural language or coordinates\n\n        Args:\n            goal_description: Natural language description of goal\n            goal_pose: Optional explicit goal pose [x, y, theta]\n        \"\"\"\n        self.get_logger().info(f\"Navigating to: {goal_description}\")\n\n        # If only description provided, use VLA to understand\n        if goal_pose is None:\n            goal_pose = self.vision_system.localize_goal_from_description(\n                goal_description, self.current_pose\n            )\n\n            if goal_pose is None:\n                self.status_pub.publish(String(data=\"Cannot locate goal\"))\n                return\n\n        self.current_goal = goal_pose\n\n        # Plan global path\n        self.current_path = self.path_planner.plan_path(\n            self.current_pose[:2], goal_pose[:2], self.obstacles\n        )\n\n        if not self.current_path:\n            self.status_pub.publish(String(data=\"No path found\"))\n            return\n\n        # Publish path for visualization\n        self.publish_path(self.current_path)\n\n        # Start navigation\n        self.navigating = True\n        self.status_pub.publish(String(data=\"Navigating\"))\n\n    def navigation_loop(self):\n        \"\"\"Main navigation control loop\"\"\"\n        if not self.navigating or not self.current_path:\n            return\n\n        # Get current state\n        current_pos = self.current_pose[:2]\n        current_vel = self.get_current_velocity()\n        current_scan = self.get_latest_scan()\n\n        # Check if reached goal\n        goal_distance = np.linalg.norm(self.current_goal[:2] - current_pos)\n        if goal_distance < 0.2:  # Within 20cm\n            self.reach_goal()\n            return\n\n        # Local path planning considering obstacles\n        local_goal = self.get_next_waypoint()\n        local_path = self.local_planner.plan_local_path(\n            current_pos, current_vel, local_goal, current_scan\n        )\n\n        # Generate walking command\n        cmd_vel = self.generate_walking_command(local_path, current_vel)\n\n        # Check for dynamic obstacles\n        if self.detect_dynamic_obstacles(current_scan):\n            cmd_vel = self.avoid_obstacle(cmd_vel, current_scan)\n\n        # Publish command\n        self.publish_command(cmd_vel)\n\n    def generate_walking_command(self, local_path: List[np.ndarray],\n                               current_vel: np.ndarray) -> Twist:\n        \"\"\"Generate walking command from local path\"\"\"\n        if not local_path:\n            return Twist()  # Stop\n\n        # Get target point\n        target = local_path[0]\n        dx = target[0] - self.current_pose[0]\n        dy = target[1] - self.current_pose[1]\n\n        # Calculate desired velocity\n        distance = np.sqrt(dx**2 + dy**2)\n        if distance < 0.1:\n            return Twist()\n\n        # Convert to robot frame\n        cos_theta = np.cos(self.current_pose[2])\n        sin_theta = np.sin(self.current_pose[2])\n\n        cmd_x = dx * cos_theta + dy * sin_theta\n        cmd_y = -dx * sin_theta + dy * cos_theta\n\n        # Create command\n        cmd = Twist()\n        cmd.linear.x = np.clip(cmd_x * 2.0, -0.5, 0.5)  # Max 0.5 m/s\n        cmd.angular.z = np.clip(np.arctan2(cmd_y, cmd_x), -1.0, 1.0)\n\n        return cmd\n\n    def handle_dynamic_obstacle(self, obstacle_detection: Dict):\n        \"\"\"Handle dynamic obstacle detection\"\"\"\n        obstacle_type = obstacle_detection['type']\n        obstacle_pose = obstacle_detection['pose']\n        obstacle_vel = obstacle_detection['velocity']\n\n        if obstacle_type == 'person':\n            # Predict person trajectory\n            predicted_path = self.predict_person_trajectory(obstacle_pose, obstacle_vel)\n\n            # Adjust path to avoid collision\n            if self.will_collide_with_path(predicted_path):\n                self.replan_path_around_obstacle(obstacle_detection)\n\n        elif obstacle_type == 'moving_object':\n            # Wait or go around\n            self.handle_moving_object(obstacle_detection)\n\n    def reach_goal(self):\n        \"\"\"Handle reaching navigation goal\"\"\"\n        self.navigating = False\n        self.status_pub.publish(String(data=\"Goal reached\"))\n\n        # Notify task completion\n        completion_msg = String()\n        completion_msg.data = f\"Reached goal: {self.current_goal}\"\n        self.create_publisher(String, '/task/completion', 10).publish(completion_msg)\n\n    def adapt_to_terrain(self, terrain_analysis: Dict):\n        \"\"\"Adapt walking parameters to terrain\"\"\"\n        terrain_type = terrain_analysis['type']\n        roughness = terrain_analysis['roughness']\n        slope = terrain_analysis['slope']\n\n        # Adjust walking parameters\n        if terrain_type == 'uneven':\n            # Reduce speed, increase step height\n            self.walking_parameters['step_height'] = 0.1\n            self.walking_parameters['walking_speed'] = 0.2\n        elif terrain_type == 'slippery':\n            # Shorter steps, wider stance\n            self.walking_parameters['step_length'] = 0.15\n            self.walking_parameters['step_width'] = 0.25\n        elif abs(slope) > 0.1:  # Sloped terrain\n            # Lean into slope\n            self.walking_parameters['torso_lean'] = slope * 0.5\n\nclass VisionLanguageActionSystem:\n    \"\"\"VLA system for understanding navigation commands\"\"\"\n\n    def __init__(self):\n        # Load pretrained VLA model\n        self.vla_model = torch.load('models/vla_navigation.pth')\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n        # Spatial reasoning\n        self.spatial_reasoner = SpatialReasoner()\n\n    def interpret_command(self, command: str,\n                          robot_pose: np.ndarray) -> Dict:\n        \"\"\"\n        Interpret natural language navigation command\n\n        Args:\n            command: Natural language command\n            robot_pose: Current robot pose\n\n        Returns:\n            Parsed navigation instruction\n        \"\"\"\n        # Tokenize and encode command\n        inputs = self.tokenizer(\n            command,\n            return_tensors='pt',\n            padding=True,\n            truncation=True\n        )\n\n        # Extract intent and entities\n        with torch.no_grad():\n            outputs = self.vla_model(**inputs)\n\n        # Parse instruction\n        instruction = {\n            'action': self.extract_action(outputs),\n            'object': self.extract_object(outputs),\n            'location': self.extract_location(outputs),\n            'spatial_relation': self.extract_spatial_relation(outputs)\n        }\n\n        # Ground to environment\n        instruction = self.ground_to_environment(instruction, robot_pose)\n\n        return instruction\n\n    def ground_to_environment(self, instruction: Dict,\n                             robot_pose: np.ndarray) -> Dict:\n        \"\"\"Ground instruction to actual environment\"\"\"\n        if instruction['object'] and instruction['location']:\n            # Find object location\n            obj_pose = self.locate_object(\n                instruction['object'],\n                robot_pose\n            )\n\n            if obj_pose is not None:\n                instruction['goal_pose'] = obj_pose\n            else:\n                instruction['goal_pose'] = None\n\n        elif instruction['location']:\n            # Convert location description to pose\n            instruction['goal_pose'] = self.convert_location_to_pose(\n                instruction['location'],\n                instruction['spatial_relation'],\n                robot_pose\n            )\n\n        return instruction\n\n    def locate_object(self, object_name: str,\n                     robot_pose: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Locate object in environment\"\"\"\n        # Query object database\n        objects = self.query_object_database()\n\n        for obj in objects:\n            if obj['name'] == object_name:\n                # Check if object is visible and reachable\n                if self.is_object_visible(obj, robot_pose):\n                    return obj['pose']\n\n        return None\n"})})}),"\n",(0,a.jsx)(n.h3,{id:"2-task-planning-and-execution",children:"2. Task Planning and Execution"}),"\n",(0,a.jsx)(r.A,{title:"Task Planning System",language:"python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom typing import List, Dict, Optional, Any\nfrom enum import Enum\nimport json\n\nclass TaskType(Enum):\n    NAVIGATE = "navigate"\n    PICK = "pick"\n    PLACE = "place"\n    INTERACT = "interact"\n    WAIT = "wait"\n    SPEAK = "speak"\n\nclass Task:\n    """Task representation"""\n    def __init__(self, task_type: TaskType, parameters: Dict,\n                 priority: int = 0):\n        self.type = task_type\n        self.parameters = parameters\n        self.priority = priority\n        self.status = "pending"\n        self.dependencies = []\n\nclass TaskPlanner:\n    """Hierarchical task planner for humanoid robot"""\n\n    def __init__(self):\n        self.task_queue = []\n        self.current_task = None\n        self.task_templates = self.load_task_templates()\n\n        # Task execution knowledge\n        self.execution_modules = {\n            TaskType.NAVIGATE: NavigationExecutor(),\n            TaskType.PICK: PickExecutor(),\n            TaskType.PLACE: PlaceExecutor(),\n            TaskType.INTERACT: InteractionExecutor(),\n            TaskType.WAIT: WaitExecutor(),\n            TaskType.SPEAK: SpeakExecutor()\n        }\n\n    def load_task_templates(self) -> Dict:\n        """Load task templates from file"""\n        return {\n            "serve_drink": [\n                Task(TaskType.NAVIGATE, {"location": "counter"}),\n                Task(TaskType.PICK, {"object": "cup", "location": "counter"}),\n                Task(TaskType.NAVIGATE, {"location": "person"}),\n                Task(TaskType.PLACE, {"object": "cup", "location": "person"}),\n                Task(TaskType.SPEAK, {"message": "Here is your drink"})\n            ],\n            "clean_room": [\n                Task(TaskType.NAVIGATE, {"location": "room"}),\n                Task(TaskType.PICK, {"object": "trash", "location": "floor"}),\n                Task(TaskType.NAVIGATE, {"location": "trash_can"}),\n                Task(TaskType.PLACE, {"object": "trash", "location": "trash_can"}),\n                Task(TaskType.NAVIGATE, {"location": "room"}),\n                Task(TaskType.PICK, {"object": "items", "location": "table"}),\n                Task(TaskType.PLACE, {"object": "items", "location": "shelf"})\n            ]\n        }\n\n    def plan_from_command(self, command: str, robot_state: Dict) -> List[Task]:\n        """\n        Plan tasks from natural language command\n\n        Args:\n            command: Natural language command\n            robot_state: Current robot state\n\n        Returns:\n            List of tasks to execute\n        """\n        # Parse command to understand intent\n        intent = self.parse_command(command)\n\n        # Generate task sequence\n        if intent[\'action\'] == \'bring\':\n            tasks = self.plan_bring_task(intent, robot_state)\n        elif intent[\'action\'] == \'clean\':\n            tasks = self.plan_clean_task(intent, robot_state)\n        elif intent[\'action\'] == \'find\':\n            tasks = self.plan_find_task(intent, robot_state)\n        else:\n            # Default to simple navigation\n            tasks = [\n                Task(TaskType.NAVIGATEATE, intent[\'parameters\'])\n            ]\n\n        # Optimize task sequence\n        tasks = self.optimize_task_sequence(tasks, robot_state)\n\n        return tasks\n\n    def plan_bring_task(self, intent: Dict, robot_state: Dict) -> List[Task]:\n        """Plan bring object task"""\n        object_name = intent[\'object\']\n        target_location = intent[\'target\']\n\n        tasks = [\n            # Navigate to object location\n            Task(TaskType.NAVIGATE, {\n                "location": intent[\'object_location\']\n            }),\n\n            # Pick up object\n            Task(TaskType.PICK, {\n                "object": object_name,\n                "location": intent[\'object_location\']\n            }),\n\n            # Navigate to target location\n            Task(TaskType.NAVIGATE, {\n                "location": target_location\n            }),\n\n            # Place object\n            Task(TaskType.PLACE, {\n                "object": object_name,\n                "location": target_location\n            })\n        ]\n\n        # Add dependencies\n        tasks[1].dependencies.append(tasks[0])  # Pick after navigation\n        tasks[2].dependencies.append(tasks[1])  # Navigate after pick\n        tasks[3].dependencies.append(tasks[2])  # Place after navigation\n\n        return tasks\n\n    def execute_next_task(self, robot_state: Dict) -> bool:\n        """\n        Execute next available task\n\n        Args:\n            robot_state: Current robot state\n\n        Returns:\n            True if task was executed\n        """\n        # Find next executable task\n        next_task = self.find_executable_task()\n\n        if next_task is None:\n            if self.current_task is None:\n                # No tasks left\n                return False\n            else:\n                # Continue current task\n                next_task = self.current_task\n\n        # Check if we need to switch tasks\n        if self.current_task is not None and self.current_task != next_task:\n            self.pause_current_task()\n\n        # Execute task\n        success = self.execute_task(next_task, robot_state)\n\n        if success:\n            next_task.status = "completed"\n            self.current_task = None\n        else:\n            next_task.status = "failed"\n            self.handle_task_failure(next_task)\n\n        return True\n\n    def execute_task(self, task: Task, robot_state: Dict) -> bool:\n        """Execute individual task"""\n        if task.type not in self.execution_modules:\n            return False\n\n        self.current_task = task\n        task.status = "executing"\n\n        # Get executor\n        executor = self.execution_modules[task.type]\n\n        # Check preconditions\n        if not executor.check_preconditions(task, robot_state):\n            return False\n\n        # Execute task\n        success = executor.execute(task, robot_state)\n\n        return success\n\n    def optimize_task_sequence(self, tasks: List[Task],\n                               robot_state: Dict) -> List[Task]:\n        """Optimize task sequence for efficiency"""\n        # This would use a planning algorithm\n        # Simplified implementation\n\n        # Sort by priority\n        tasks.sort(key=lambda t: t.priority, reverse=True)\n\n        # Optimize navigation tasks (combine nearby destinations)\n        optimized_tasks = []\n        nav_tasks = [t for t in tasks if t.type == TaskType.NAVIGATE]\n        other_tasks = [t for t in tasks if t.type != TaskType.NAVIGATE]\n\n        # Keep original order for non-navigation tasks\n        optimized_tasks.extend(other_tasks)\n\n        # Insert navigation tasks at optimal points\n        for nav_task in nav_tasks:\n            # Find best insertion point\n            best_idx = self.find_best_navigation_insertion(\n                nav_task, optimized_tasks, robot_state\n            )\n            optimized_tasks.insert(best_idx, nav_task)\n\n        return optimized_tasks\n\nclass TaskExecutor:\n    """Base class for task executors"""\n\n    def check_preconditions(self, task: Task, robot_state: Dict) -> bool:\n        """Check if task preconditions are met"""\n        return True\n\n    def execute(self, task: Task, robot_state: Dict) -> bool:\n        """Execute task"""\n        return True\n\n    def cleanup(self, task: Task):\n        """Clean up after task execution"""\n        pass\n\nclass PickExecutor(TaskExecutor):\n    """Executor for pick tasks"""\n\n    def execute(self, task: Task, robot_state: Dict) -> bool:\n        """Execute pick task"""\n        object_name = task.parameters[\'object\']\n        target_location = task.parameters[\'location\']\n\n        # Navigate to object\n        self.navigate_to_location(target_location)\n\n        # Detect and localize object\n        object_pose = self.detect_object(object_name)\n        if object_pose is None:\n            return False\n\n        # Plan approach trajectory\n        approach_trajectory = self.plan_approach(object_pose)\n\n        # Execute grasp\n        success = self.execute_grasp(object_pose, approach_trajectory)\n\n        # Verify grasp\n        if success:\n            success = self.verify_grasp(object_name)\n\n        return success\n\nclass InteractionExecutor(TaskExecutor):\n    """Executor for human interaction tasks"""\n\n    def execute(self, task: Task, robot_state: Dict) -> bool:\n        """Execute interaction task"""\n        interaction_type = task.parameters.get(\'type\', \'greet\')\n        person_id = task.parameters.get(\'person_id\')\n\n        if interaction_type == \'greet\':\n            return self.execute_greeting(person_id)\n        elif interaction_type == \'handshake\':\n            return self.execute_handshake(person_id)\n        elif interaction_type == \'conversation\':\n            return self.execute_conversation(task.parameters.get(\'topic\'))\n\n        return False\n\n    def execute_greeting(self, person_id: str) -> bool:\n        """Execute greeting interaction"""\n        # Detect person\n        person_pose = self.detect_person(person_id)\n        if person_pose is None:\n            return False\n\n        # Face person\n        self.face_person(person_pose)\n\n        # Greet verbally\n        greeting_message = self.generate_greeting_message()\n        self.speak(greeting_message)\n\n        # Perform gesture (wave)\n        self.perform_gesture(\'wave\')\n\n        return True\n\n    def generate_greeting_message(self) -> str:\n        """Generate contextually appropriate greeting"""\n        hour = self.get_current_hour()\n\n        if hour < 12:\n            return "Good morning! How can I help you today?"\n        elif hour < 18:\n            return "Good afternoon! How may I assist you?"\n        else:\n            return "Good evening! I\'m here to help."\n\nclass ConversationManager:\n    """Manages conversations with humans"""\n\n    def __init__(self):\n        self.conversation_history = []\n        self.current_topic = None\n\n    def process_user_input(self, user_input: str, context: Dict) -> str:\n        """\n        Process user input and generate response\n\n        Args:\n            user_input: User\'s speech/text input\n            context: Current context (person, location, etc.)\n\n        Returns:\n            Robot\'s response\n        """\n        # Store in conversation history\n        self.conversation_history.append({\n            \'speaker\': \'user\',\n            \'message\': user_input,\n            \'timestamp\': self.get_timestamp(),\n            \'context\': context\n        })\n\n        # Analyze user input\n        intent = self.analyze_intent(user_input)\n        entities = self.extract_entities(user_input)\n\n        # Generate response\n        if intent == \'question\':\n            response = self.answer_question(entities, context)\n        elif intent == \'request\':\n            response = self.handle_request(entities, context)\n        elif intent == \'greeting\':\n            response = self.respond_to_greeting()\n        else:\n            response = self.generate_generic_response()\n\n        # Store response\n        self.conversation_history.append({\n            \'speaker\': \'robot\',\n            \'message\': response,\n            \'timestamp\': self.get_timestamp(),\n            \'context\': context\n        })\n\n        return response\n\n    def answer_question(self, entities: Dict, context: Dict) -> str:\n        """Answer user\'s question"""\n        question_type = entities.get(\'question_type\')\n\n        if question_type == \'what_can_you_do\':\n            return self.list_capabilities()\n        elif question_type == \'where_is\':\n            object_name = entities.get(\'object\')\n            location = self.find_object_location(object_name)\n            return f"The {object_name} is in the {location}"\n        elif question_type == \'how_to\':\n            return self.provide_instructions(entities)\n        else:\n            return "I\'m not sure how to answer that. Can you rephrase your question?"\n\n    def handle_request(self, entities: Dict, context: Dict) -> str:\n        """Handle user\'s request"""\n        action = entities.get(\'action\')\n        object_name = entities.get(\'object\')\n\n        if action == \'bring\':\n            # Create task to bring object\n            self.create_bring_task(object_name, context)\n            return f"I\'ll bring the {object_name} to you."\n        elif action == \'clean\':\n            # Create cleaning task\n            self.create_clean_task(entities, context)\n            return "I\'ll start cleaning right away."\n        elif action == \'find\':\n            # Create search task\n            self.create_search_task(object_name, context)\n            return f"I\'ll look for the {object_name}."\n        else:\n            return "I\'m not sure I can do that. Please be more specific."\n\n    def maintain_context(self, new_context: Dict):\n        """Update and maintain conversation context"""\n        # Update person location\n        if \'person_pose\' in new_context:\n            self.context[\'last_person_pose\'] = new_context[\'person_pose\']\n\n        # Update task status\n        if \'current_task\' in new_context:\n            self.context[\'current_task\'] = new_context[\'current_task\']\n\n        # Update environment state\n        if \'environment_changes\' in new_context:\n            self.context[\'environment\'].update(new_context[\'environment_changes\'])\n'})})}),"\n",(0,a.jsx)(n.h3,{id:"3-integrated-control-system",children:"3. Integrated Control System"}),"\n",(0,a.jsx)(r.A,{title:"Integrated Control System",language:"python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom typing import Dict, List, Optional\nimport time\n\nclass IntegratedControlSystem:\n    """Integrated control system for autonomous humanoid robot"""\n\n    def __init__(self):\n        # Initialize all subsystems\n        self.navigation = AutonomousNavigator()\n        self.manipulation = WholeBodyManipulator(None)\n        self.balance = ReactiveBalanceController()\n        self.task_planner = TaskPlanner()\n        self.conversation = ConversationManager()\n        self.perception = PerceptionSystem()\n\n        # Control modes\n        self.control_mode = \'autonomous\'  # \'autonomous\', \'teleoperated\', \'safe\'\n\n        # System state\n        self.current_state = {\n            \'pose\': np.zeros(3),\n            \'velocity\': np.zeros(6),\n            \'joint_positions\': np.zeros(25),\n            \'joint_velocities\': np.zeros(25),\n            \'held_object\': None,\n            \'battery_level\': 100.0,\n            \'emergency_stop\': False\n        }\n\n        # Control loop\n        self.control_rate = 100  # Hz\n        self.last_control_time = 0\n\n    def start_autonomous_operation(self):\n        """Start autonomous operation loop"""\n        print("Starting autonomous humanoid robot system...")\n\n        while not self.should_shutdown():\n            current_time = time.time()\n\n            # Control loop at specified rate\n            if current_time - self.last_control_time >= 1.0 / self.control_rate:\n                self.control_loop()\n                self.last_control_time = current_time\n\n    def control_loop(self):\n        """Main control loop"""\n        try:\n            # Update perception\n            self.update_perception()\n\n            # Execute tasks\n            self.execute_tasks()\n\n            # Maintain balance\n            self.maintain_balance()\n\n            # Update state\n            self.update_state()\n\n        except Exception as e:\n            self.handle_control_error(e)\n\n    def execute_tasks(self):\n        """Execute planned tasks"""\n        # Check for new commands\n        if self.has_new_command():\n            self.process_new_command()\n\n        # Execute next task\n        if self.task_planner.execute_next_task(self.current_state):\n            # Task was executed\n            pass\n        else:\n            # No task to execute, maintain default behavior\n            self.maintain_default_behavior()\n\n    def maintain_balance(self):\n        """Maintain balance during all operations"""\n        # Get current balance state\n        com_state = self.get_com_state()\n        support_polygon = self.get_support_polygon()\n\n        # Check stability\n        if not self.is_stable(com_state, support_polygon):\n            # Trigger balance recovery\n            self.recover_balance()\n        else:\n            # Apply continuous balance control\n            self.apply_balance_control()\n\n    def handle_emergency(self, emergency_type: str, details: Dict):\n        """Handle emergency situations"""\n        self.control_mode = \'safe\'\n\n        if emergency_type == \'collision\':\n            self.stop_all_motion()\n            self.check_for_damage()\n\n        elif emergency_type == \'fall\':\n            self.execute_fall_recovery()\n\n        elif emergency_type == \'battery_low\':\n            self.navigate_to_charging_station()\n\n        elif emergency_type == \'lost_balance\':\n            self.execute_emergency_balance_recovery()\n\n    def learn_from_experience(self):\n        """Learn and adapt from experience"""\n        # Collect experience data\n        experience = self.collect_experience()\n\n        # Update models\n        self.update_navigation_model(experience)\n        self.update_manipulation_model(experience)\n        self.update_interaction_model(experience)\n\n    def demonstrate_capabilities(self):\n        """Demonstrate integrated capabilities"""\n        print("\\n=== Capability Demonstration ===")\n\n        # Scenario 1: Serve a drink\n        print("\\n1. Serving drink scenario...")\n        self.demonstrate_serving_drink()\n\n        # Scenario 2: Clean and organize\n        print("\\n2. Cleaning scenario...")\n        self.demonstrate_cleaning()\n\n        # Scenario 3: Interactive assistance\n        print("\\n3. Interactive assistance...")\n        self.demonstrate_interaction()\n\n        # Scenario 4: Complex navigation\n        print("\\n4. Complex navigation scenario...")\n        self.demonstrate_navigation()\n\n    def demonstrate_serving_drink(self):\n        """Demonstrate serving a drink"""\n        # Start at kitchen\n        self.navigation.navigate_to_goal("kitchen counter")\n\n        # Find and pick up drink\n        cup_detected = self.perception.detect_object("cup")\n        if cup_detected:\n            self.manipulation.pick_up_object_balanced(\n                cup_detected[\'pose\'], expected_mass=0.3\n            )\n\n            # Navigate to person\n            person_location = self.conversation.request_location("Where should I serve the drink?")\n            self.navigation.navigate_to_goal("person", person_location)\n\n            # Serve with interaction\n            self.conversation.process_user_input(\n                "Here is your drink, enjoy!",\n                {\'context\': \'serving\'}\n            )\n\n            # Place drink\n            self.manipulation.place_object(person_location)\n\n    def demonstrate_cleaning(self):\n        """Demonstrate cleaning capabilities"""\n        # Scan room for objects to clean\n        room_state = self.perception.scan_room()\n\n        # Create cleaning plan\n        cleaning_plan = self.plan_cleaning(room_state)\n\n        # Execute cleaning\n        for item in cleaning_plan:\n            if item[\'type\'] == \'trash\':\n                self.pick_up_and_dispose(item)\n            elif item[\'type\'] == \'misplaced\':\n                self.pick_up_and_organize(item)\n\n    def demonstrate_interaction(self):\n        """Demonstrate human interaction"""\n        # Greet person\n        self.conversation.process_user_input(\n            "Hello! I\'m here to help.",\n            {\'action\': \'greet\'}\n        )\n\n        # Wait for and process response\n        user_input = self.wait_for_user_input()\n        response = self.conversation.process_user_input(\n            user_input,\n            {\'context\': \'conversation\'}\n        )\n\n        # Perform requested action\n        if \'bring\' in user_input:\n            self.handle_bring_request(user_input)\n        elif \'help\' in user_input:\n            self.provide_assistance()\n        else:\n            self.respond_to_query(user_input)\n\nclass PerformanceEvaluator:\n    """Evaluate system performance"""\n\n    def __init__(self):\n        self.metrics = {\n            \'navigation_success\': [],\n            \'manipulation_success\': [],\n            \'interaction_naturalness\': [],\n            \'energy_efficiency\': [],\n            \'task_completion_time\': []\n        }\n\n    def evaluate_task_performance(self, task_type: str,\n                                  success: bool,\n                                  time_taken: float,\n                                  energy_used: float):\n        """Evaluate performance of completed task"""\n        self.metrics[\'task_completion_time\'].append(time_taken)\n        self.metrics[\'energy_efficiency\'].append(energy_used)\n\n        if task_type == \'navigation\':\n            self.metrics[\'navigation_success\'].append(success)\n        elif task_type == \'manipulation\':\n            self.metrics[\'manipulation_success\'].append(success)\n        elif task_type == \'interaction\':\n            # Would need human evaluation\n            pass\n\n    def generate_performance_report(self) -> Dict:\n        """Generate comprehensive performance report"""\n        report = {\n            \'navigation_success_rate\': np.mean(self.metrics[\'navigation_success\']) if self.metrics[\'navigation_success\'] else 0,\n            \'manipulation_success_rate\': np.mean(self.metrics[\'manipulation_success\']) if self.metrics[\'manipulation_success\'] else 0,\n            \'average_task_time\': np.mean(self.metrics[\'task_completion_time\']) if self.metrics[\'task_completion_time\'] else 0,\n            \'energy_efficiency\': np.mean(self.metrics[\'energy_efficiency\']) if self.metrics[\'energy_efficiency\'] else 0\n        }\n\n        return report\n\n# Demo\ndef main():\n    print("=== Autonomous Humanoid Robot Capstone Demo ===")\n\n    # Initialize system\n    robot = IntegratedControlSystem()\n    evaluator = PerformanceEvaluator()\n\n    # Demonstrate capabilities\n    robot.demonstrate_capabilities()\n\n    # Performance evaluation\n    report = evaluator.generate_performance_report()\n\n    print("\\n=== Performance Report ===")\n    print(f"Navigation Success Rate: {report[\'navigation_success_rate\']*100:.1f}%")\n    print(f"Manipulation Success Rate: {report[\'manipulation_success_rate\']*100:.1f}%")\n    print(f"Average Task Time: {report[\'average_task_time\']:.2f}s")\n    print(f"Energy Efficiency: {report[\'energy_efficiency\']:.2f}")\n\n    # Learning summary\n    print("\\n=== Learning Achievements ===")\n    print("\u2713 Integrated perception-action loops")\n    print("\u2713 Natural language understanding and response")\n    print("\u2713 Dynamic balance during manipulation")\n    print("\u2713 Adaptive task planning")\n    print("\u2713 Human-robot interaction")\n\nif __name__ == "__main__":\n    main()\n'})})}),"\n",(0,a.jsx)(n.h2,{id:"lab-exercise-complete-system-integration",children:"Lab Exercise: Complete System Integration"}),"\n",(0,a.jsxs)("div",{className:"lab-exercise",children:[(0,a.jsx)(n.h3,{id:"objective",children:"Objective"}),(0,a.jsx)(n.p,{children:"Integrate all course components into a complete autonomous humanoid robot system capable of performing service tasks in a home environment."}),(0,a.jsx)(n.h3,{id:"setup",children:"Setup"}),(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Complete robot model with all sensors and actuators"}),"\n",(0,a.jsx)(n.li,{children:"Integrated software stack (ROS 2, Isaac Sim, VLA)"}),"\n",(0,a.jsx)(n.li,{children:"Home environment simulation"}),"\n",(0,a.jsx)(n.li,{children:"Task planning and execution system"}),"\n"]}),(0,a.jsx)(n.h3,{id:"implementation",children:"Implementation"}),(0,a.jsx)(r.A,{language:"python",editable:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# autonomous_humanoid_system.py\nimport numpy as np\nimport time\nfrom typing import Dict, List, Optional\n\nclass AutonomousHumanoidSystem:\n    """Complete autonomous humanoid robot system"""\n\n    def __init__(self):\n        print("Initializing Autonomous Humanoid Robot System...")\n\n        # Core systems\n        self.perception = IntegratedPerceptionSystem()\n        self.cognition = CognitiveSystem()\n        self.action = ActionSystem()\n        self.learning = LearningSystem()\n\n        # System state\n        self.state = SystemState()\n        self.goals = GoalManager()\n\n        # Performance tracking\n        self.performance = PerformanceTracker()\n\n        print("System initialization complete!")\n\n    def start_autonomous_operation(self):\n        """Start fully autonomous operation"""\n        print("\\n=== Starting Autonomous Operation ===")\n        print("The robot will now operate autonomously...")\n        print("Press Ctrl+C to stop\\n")\n\n        try:\n            while self.should_continue_operation():\n                # Perception cycle\n                perception_result = self.perception.perceive(self.state)\n\n                # Cognition cycle\n                goals, actions = self.cognition.think(\n                    perception_result, self.state, self.goals\n                )\n\n                # Action cycle\n                action_result = self.action.execute(actions, self.state)\n\n                # Learning cycle\n                self.learning.learn(\n                    perception_result, goals, actions, action_result\n                )\n\n                # Update state\n                self.state.update(perception_result, action_result)\n\n                # Track performance\n                self.performance.update(goals, action_result)\n\n                # Cycle delay\n                time.sleep(0.1)  # 10 Hz operation\n\n        except KeyboardInterrupt:\n            print("\\n\\nStopping autonomous operation...")\n            self.shutdown_sequence()\n\n    def demonstrate_full_capabilities(self):\n        """Demonstrate all integrated capabilities"""\n        print("\\n=== Full Capability Demonstration ===")\n\n        # Morning routine\n        self.demonstrate_morning_routine()\n\n        # Service tasks\n        self.demonstrate_service_tasks()\n\n        # Emergency response\n        self.demonstrate_emergency_handling()\n\n        # Learning and adaptation\n        self.demonstrate_learning_adaptation()\n\n    def demonstrate_morning_routine(self):\n        """Demonstrate morning household routine"""\n        print("\\n1. Morning Routine Demonstration:")\n\n        # Wake up sequence\n        self.action.wake_up_sequence()\n        self.state.update_time("07:00")\n\n        # Navigate to kitchen\n        self.action.navigate_to("kitchen")\n\n        # Check calendar\n        schedule = self.cognition.check_calendar()\n        print(f"   Today\'s schedule: {schedule}")\n\n        # Make coffee\n        coffee_success = self.make_coffee()\n        if coffee_success:\n            print("   \u2713 Coffee prepared successfully")\n\n        # Serve breakfast\n        self.serve_breakfast()\n\n        # Clean up\n        self.clean_kitchen()\n\n    def demonstrate_service_tasks(self):\n        """Demonstrate various service tasks"""\n        print("\\n2. Service Tasks Demonstration:")\n\n        tasks = [\n            "Organize the living room",\n            "Water the plants",\n            "Bring the mail",\n            "Tidy up the bedroom"\n        ]\n\n        for task in tasks:\n            print(f"\\n   Executing: {task}")\n            success = self.execute_service_task(task)\n            if success:\n                print(f"   \u2713 Completed: {task}")\n            else:\n                print(f"   \u2717 Failed: {task}")\n\n    def make_coffee(self) -> bool:\n        """Make coffee using learned recipe"""\n        # Get coffee machine location\n        machine_pos = self.perception.locate_object("coffee_machine")\n        if machine_pos is None:\n            print("   Coffee machine not found")\n            return False\n\n        # Navigate to coffee machine\n        self.action.navigate_to_position(machine_pos)\n\n        # Detect coffee supplies\n        supplies = self.perception.detect_objects_nearby(\n            ["coffee_beans", "water", "mug"]\n        )\n\n        if not all(supply in supplies for supply in ["coffee_beans", "mug"]):\n            print("   Missing coffee supplies")\n            return False\n\n        # Execute coffee making sequence\n        steps = [\n            "grind_beans",\n            "heat_water",\n            "brew_coffee",\n            "pour_into_mug"\n        ]\n\n        for step in steps:\n            print(f"   {step.replace(\'_\', \' \').title()}...")\n            self.action.execute_primitive(step)\n            time.sleep(0.5)  # Simulate action time\n\n        return True\n\n    def serve_breakfast(self):\n        """Serve breakfast to resident"""\n        # Find resident\n        resident = self.perception.locate_person("resident")\n        if resident is None:\n            print("   Resident not found")\n            return\n\n        # Navigate with coffee\n        self.action.navigate_to_person(resident, carrying="coffee")\n\n        # Serve with greeting\n        greeting = self.cognition.generate_contextual_greeting(\n            time_of_day="morning"\n        )\n        self.action.speak(greeting)\n        self.action.place_object("coffee", resident["table_position"])\n\n    def demonstrate_emergency_handling(self):\n        """Demonstrate emergency response capabilities"""\n        print("\\n3. Emergency Handling Demonstration:")\n\n        # Simulate different emergencies\n        emergencies = [\n            {"type": "fall_detected", "location": "bedroom"},\n            {"type": "fire_alarm", "location": "kitchen"},\n            {"type": "intruder", "location": "entrance"},\n            {"type": "medical_emergency", "location": "living_room"}\n        ]\n\n        for emergency in emergencies:\n            print(f"\\n   Handling: {emergency[\'type\']}")\n            self.handle_emergency(emergency)\n\n    def handle_emergency(self, emergency: Dict):\n        """Handle specific emergency situation"""\n        if emergency["type"] == "fall_detected":\n            self.handle_fall_emergency(emergency)\n        elif emergency["type"] == "fire_alarm":\n            self.handle_fire_emergency(emergency)\n        elif emergency["type"] == "intruder":\n            self.handle_intruder_emergency(emergency)\n        elif emergency["type"] == "medical_emergency":\n            self.handle_medical_emergency(emergency)\n\n    def handle_fall_emergency(self, emergency: Dict):\n        """Handle fall detection emergency"""\n        print("   Fall detected! Initiating emergency protocol...")\n\n        # Navigate to location\n        self.action.emergency_navigate_to(emergency["location"])\n\n        # Assess situation\n        person_state = self.perception.assess_person_state()\n        print(f"   Person state: {person_state}")\n\n        # Call for help\n        self.action.emergency_call(\n            contact="emergency_services",\n            message=f"Fall detected at {emergency[\'location\']}"\n        )\n\n        # Provide assistance\n        if person_state["conscious"]:\n            self.action.speak("I\'m here to help. Are you okay?")\n            self.action.offer_assistance()\n        else:\n            self.action.perform_first_aid()\n            self.action.monitor_vitals()\n\n    def demonstrate_learning_adaptation(self):\n        """Demonstrate learning and adaptation capabilities"""\n        print("\\n4. Learning and Adaptation Demonstration:"")\n\n        # Learn from mistakes\n        self.learn_from_failed_task()\n\n        # Adapt to user preferences\n        self.adapt_to_preferences()\n\n        # Optimize routines\n        self.optimize_daily_routines()\n\n    def learn_from_failed_task(self):\n        """Learn from failed task execution"""\n        print("   Learning from recent failures...")\n\n        # Get recent failures\n        failures = self.performance.get_recent_failures()\n\n        for failure in failures:\n            # Analyze failure\n            cause = self.learning.analyze_failure(failure)\n\n            # Update strategy\n            if cause == "object_not_found":\n                self.learning.update_object_locations()\n            elif cause == "balance_lost":\n                self.learning.adjust_balance_parameters()\n            elif cause == "grasp_failed":\n                self.learning.update_grasp_strategy()\n\n            print(f"   Learned: {failure[\'task\']} - {cause}")\n\n    def adapt_to_preferences(self):\n        """Adapt behavior to user preferences"""\n        print("   Adapting to user preferences...")\n\n        # Get preference data\n        preferences = self.learning.get_user_preferences()\n\n        # Adapt behavior\n        if preferences.get("morning_person", False):\n            self.personality.set_morning_mode(0.8)\n        if preferences.get("quiet_mornings", True):\n            self.action.set_noise_level("low", hours=(6, 10))\n        if preferences.get("likes_coffee", True):\n            self.routines.add_to_morning("make_coffee", priority=1)\n\n        print("   Preferences adapted successfully")\n\n    def generate_system_report(self):\n        """Generate comprehensive system performance report"""\n        print("\\n=== System Performance Report ===")\n\n        # Performance metrics\n        metrics = self.performance.get_comprehensive_metrics()\n\n        print(f"\\nTask Completion Rate: {metrics[\'success_rate\']*100:.1f}%")\n        print(f"Average Task Time: {metrics[\'avg_time\']:.2f} seconds")\n        print(f"Energy Efficiency: {metrics[\'energy_score\']:.2f}/10")\n        print(f"User Satisfaction: {metrics[\'satisfaction\']*100:.1f}%")\n\n        # Learning progress\n        learning_stats = self.learning.get_learning_statistics()\n        print(f"\\nTasks Learned: {learning_stats[\'tasks_learned\']}")\n        print(f"Adaptations Made: {learning_stats[\'adaptations\']}")\n        print(f"Success Improvement: {learning_stats[\'improvement\']*100:.1f}%")\n\n        # System health\n        health = self.state.get_health_report()\n        print(f"\\nSystem Health: {health[\'overall\']}")\n        print(f"Battery: {health[\'battery\']}%")\n        print(f"Joint Health: {health[\'joints\']}")\n        print(f"Sensor Health: {health[\'sensors\']}")\n\n# Supporting classes\nclass SystemState:\n    """Manage robot system state"""\n    def __init__(self):\n        self.pose = np.zeros(3)\n        self.joint_positions = np.zeros(25)\n        self.held_object = None\n        self.time = "00:00"\n        self.environment = {}\n\n    def update(self, perception_result, action_result):\n        """Update system state"""\n        self.pose = perception_result.get(\'pose\', self.pose)\n        self.joint_positions = action_result.get(\'joint_positions\', self.joint_positions)\n        self.held_object = action_result.get(\'held_object\', self.held_object)\n\n    def update_time(self, time_str):\n        """Update time"""\n        self.time = time_str\n\n    def get_health_report(self) -> Dict:\n        """Get system health report"""\n        return {\n            \'overall\': \'good\',\n            \'battery\': 85,\n            \'joints\': \'healthy\',\n            \'sensors\': \'operational\'\n        }\n\nclass GoalManager:\n    """Manage active goals"""\n    def __init__(self):\n        self.goals = []\n        self.priorities = {}\n\n    def add_goal(self, goal: Dict, priority: int = 0):\n        """Add new goal"""\n        self.goals.append(goal)\n        self.priorities[goal[\'id\']] = priority\n\n    def get_top_priority_goal(self):\n        """Get highest priority goal"""\n        if not self.goals:\n            return None\n\n        return max(self.goals, key=lambda g: self.priorities.get(g[\'id\'], 0))\n\nclass PerformanceTracker:\n    """Track system performance"""\n    def __init__(self):\n        self.task_history = []\n        self.metrics = {}\n\n    def update(self, goals, action_result):\n        """Update performance tracking"""\n        if action_result.get(\'completed\'):\n            self.task_history.append({\n                \'goal\': goals,\n                \'success\': True,\n                \'time\': action_result.get(\'time\', 0)\n            })\n\n    def get_recent_failures(self):\n        """Get recent task failures"""\n        return [t for t in self.task_history if not t.get(\'success\', True)]\n\n    def get_comprehensive_metrics(self):\n        """Get comprehensive performance metrics"""\n        if not self.task_history:\n            return {\n                \'success_rate\': 0,\n                \'avg_time\': 0,\n                \'energy_score\': 5,\n                \'satisfaction\': 0.8\n            }\n\n        successful = sum(1 for t in self.task_history if t.get(\'success\'))\n        return {\n            \'success_rate\': successful / len(self.task_history),\n            \'avg_time\': np.mean([t.get(\'time\', 0) for t in self.task_history]),\n            \'energy_score\': 8.2,\n            \'satisfaction\': 0.92\n        }\n\n# Demo execution\ndef main():\n    print("=" * 60)\n    print("AUTONOMOUS HUMANOID ROBOT SYSTEM")\n    print("Capstone Project Demonstration")\n    print("=" * 60)\n\n    # Create and configure system\n    system = AutonomousHumanoidSystem()\n\n    # Configure initial state\n    system.state.update_time("07:00")\n    system.state.pose = np.array([0.0, 0.0, 0.0])\n\n    # Run full demonstration\n    system.demonstrate_full_capabilities()\n\n    # Generate report\n    system.generate_system_report()\n\n    # Option: Start autonomous operation\n    """\n    print("\\nWould you like to start autonomous operation? (y/n)")\n    response = input("> ").lower()\n\n    if response == \'y\':\n        system.start_autonomous_operation()\n    """\n\n    print("\\n" + "=" * 60)\n    print("DEMONSTRATION COMPLETE")\n    print("=" * 60)\n\nif __name__ == "__main__":\n    main()\n'})})}),(0,a.jsx)(n.h3,{id:"expected-outcomes",children:"Expected Outcomes"}),(0,a.jsx)(n.p,{children:"The capstone system should demonstrate:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Autonomous navigation and manipulation"}),"\n",(0,a.jsx)(n.li,{children:"Natural language understanding and response"}),"\n",(0,a.jsx)(n.li,{children:"Balance maintenance during all operations"}),"\n",(0,a.jsx)(n.li,{children:"Task planning and execution"}),"\n",(0,a.jsx)(n.li,{children:"Learning and adaptation from experience"}),"\n",(0,a.jsx)(n.li,{children:"Emergency handling capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Integration of all course concepts"}),"\n"]}),(0,a.jsx)(n.h3,{id:"evaluation-criteria",children:"Evaluation Criteria"}),(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Integration Quality"})," (30%)"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"All systems work together seamlessly"}),"\n",(0,a.jsx)(n.li,{children:"Proper communication between modules"}),"\n",(0,a.jsx)(n.li,{children:"Robust error handling"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Autonomous Operation"})," (25%)"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Can operate without human intervention"}),"\n",(0,a.jsx)(n.li,{children:"Makes intelligent decisions"}),"\n",(0,a.jsx)(n.li,{children:"Handles unexpected situations"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Task Performance"})," (25%)"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Completes assigned tasks successfully"}),"\n",(0,a.jsx)(n.li,{children:"Efficient task execution"}),"\n",(0,a.jsx)(n.li,{children:"Adapts to different scenarios"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Human Interaction"})," (20%)"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Natural language understanding"}),"\n",(0,a.jsx)(n.li,{children:"Appropriate responses"}),"\n",(0,a.jsx)(n.li,{children:"Social awareness"}),"\n"]}),"\n"]}),"\n"]})]}),"\n",(0,a.jsx)(n.h2,{id:"future-enhancements",children:"Future Enhancements"}),"\n",(0,a.jsx)(n.h3,{id:"1-advanced-ai-integration",children:"1. Advanced AI Integration"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Large Language Models for complex reasoning"}),"\n",(0,a.jsx)(n.li,{children:"Reinforcement learning for skill acquisition"}),"\n",(0,a.jsx)(n.li,{children:"Transfer learning from simulation to real world"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-enhanced-sensing",children:"2. Enhanced Sensing"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Advanced tactile sensing"}),"\n",(0,a.jsx)(n.li,{children:"Emotional state recognition"}),"\n",(0,a.jsx)(n.li,{children:"Predictive perception"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-social-intelligence",children:"3. Social Intelligence"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Theory of mind implementation"}),"\n",(0,a.jsx)(n.li,{children:"Cultural awareness"}),"\n",(0,a.jsx)(n.li,{children:"Personal assistant capabilities"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Integration is key"})," - All components must work together seamlessly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Autonomy requires"})," perception, cognition, and action integration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety must be"})," designed into every aspect"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Learning enables"})," continuous improvement and adaptation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Human interaction"})," requires natural communication and social awareness"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This capstone project brings together all concepts from the course to create a complete autonomous humanoid robot system. By integrating perception, cognition, and action, we build a robot capable of performing complex tasks in human environments while maintaining safety, efficiency, and natural interaction."}),"\n",(0,a.jsx)(n.p,{children:"You've now completed the Physical AI & Humanoid Robotics course. Congratulations on building an autonomous humanoid robot system!"}),"\n",(0,a.jsx)(n.h2,{id:"final-quiz",children:"Final Quiz"}),"\n",(0,a.jsx)(i.A,{quizId:"capstone-project",questions:[{id:"q1",type:"multiple-choice",question:"What is the most critical aspect of integrating multiple robotic subsystems?",options:["Maximizing computational speed","Ensuring robust communication and proper error handling","Minimizing power consumption","Using the most advanced algorithms"],correct:1,explanation:"Robust communication between subsystems and comprehensive error handling are critical for system reliability and safety, especially in autonomous systems where failures can have serious consequences."},{id:"q2",type:"multiple-choice",question:"Which capability demonstrates true autonomy in humanoid robots?",options:["Following pre-programmed instructions","Adapting to unexpected situations without human intervention","Repeating learned tasks exactly","Operating only in controlled environments"],correct:1,explanation:"True autonomy is demonstrated when a robot can perceive unexpected situations, reason about them, and adapt its behavior without requiring human intervention."},{id:"q3",type:"true-false",question:"A successful autonomous humanoid robot should be able to learn and improve its performance over time.",correct:!0,explanation:"Learning and adaptation are essential capabilities for autonomous robots operating in dynamic human environments. This enables them to handle new situations, improve task performance, and personalize their behavior to user preferences."}]})]})}function f(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var s=t(6540);const a={},o=s.createContext(a);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);