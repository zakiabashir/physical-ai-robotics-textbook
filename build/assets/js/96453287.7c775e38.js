"use strict";(globalThis.webpackChunkphysical_ai_robotics_textbook=globalThis.webpackChunkphysical_ai_robotics_textbook||[]).push([[2859],{6219:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"chapter-3/lesson-2","title":"Lesson 3.2: Vision-Language-Action (VLA) Systems","description":"Building robots that can see, understand, and act based on language commands","source":"@site/docs/chapter-3/lesson-2.mdx","sourceDirName":"chapter-3","slug":"/chapter-3/lesson-2","permalink":"/docs/chapter-3/lesson-2","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai/physical-ai-robotics-textbook/tree/main/docs/chapter-3/lesson-2.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Lesson 3.2: Vision-Language-Action (VLA) Systems","description":"Building robots that can see, understand, and act based on language commands","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.1: NVIDIA Isaac Platform","permalink":"/docs/chapter-3/lesson-1"},"next":{"title":"Lesson 3.3: Conversational Robots","permalink":"/docs/chapter-3/lesson-3"}}');var a=t(4848),o=t(8453),i=t(2948);const r={title:"Lesson 3.2: Vision-Language-Action (VLA) Systems",description:"Building robots that can see, understand, and act based on language commands",sidebar_position:2},l="Lesson 3.2: Vision-Language-Action (VLA) Systems",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to VLA Systems",id:"introduction-to-vla-systems",level:2},{value:"Core Components",id:"core-components",level:2},{value:"1. Visual Perception Module",id:"1-visual-perception-module",level:3},{value:"2. Language Understanding Module",id:"2-language-understanding-module",level:3},{value:"3. Vision-Language Fusion",id:"3-vision-language-fusion",level:3},{value:"Training VLA Models",id:"training-vla-models",level:2},{value:"Data Collection and Preprocessing",id:"data-collection-and-preprocessing",level:3},{value:"Lab Exercise: Building a VLA-Powered Assistant",id:"lab-exercise-building-a-vla-powered-assistant",level:2},{value:"Objective",id:"objective",level:3},{value:"Setup",id:"setup",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Training the Model",id:"training-the-model",level:3},{value:"Expected Results",id:"expected-results",level:3},{value:"Real-World Applications",id:"real-world-applications",level:2},{value:"1. Home Assistants",id:"1-home-assistants",level:3},{value:"2. Manufacturing",id:"2-manufacturing",level:3},{value:"3. Healthcare",id:"3-healthcare",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Data Quality",id:"1-data-quality",level:3},{value:"2. Model Architecture",id:"2-model-architecture",level:3},{value:"3. Deployment",id:"3-deployment",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Summary",id:"summary",level:2},{value:"Quiz",id:"quiz",level:2}];function u(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components},{DiagramComponent:t,Quiz:s}=e;return t||m("DiagramComponent",!0),s||m("Quiz",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"lesson-32-vision-language-action-vla-systems",children:"Lesson 3.2: Vision-Language-Action (VLA) Systems"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)("div",{className:"learning-objectives",children:[(0,a.jsx)(e.p,{children:"After completing this lesson, you will be able to:"}),(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Explain the architecture of Vision-Language-Action (VLA) models"}),"\n",(0,a.jsx)(e.li,{children:"Implement visual grounding for robotic tasks"}),"\n",(0,a.jsx)(e.li,{children:"Build language understanding systems for robot control"}),"\n",(0,a.jsx)(e.li,{children:"Create embodied AI agents that connect perception to action"}),"\n",(0,a.jsx)(e.li,{children:"Train and deploy VLA models for real-world robotics"}),"\n"]})]}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-vla-systems",children:"Introduction to VLA Systems"}),"\n",(0,a.jsx)(e.p,{children:"Vision-Language-Action (VLA) systems represent the convergence of three critical AI capabilities:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Vision"}),": Understanding the visual world through cameras and sensors"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Language"}),": Comprehending human commands and instructions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action"}),": Executing physical tasks in the environment"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:["VLA systems enable robots to understand natural language commands like ",(0,a.jsx)(e.em,{children:'"Pick up the red cup on the table"'})," and execute them in the real world."]}),"\n",(0,a.jsx)(t,{title:"VLA System Architecture",children:(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-mermaid",children:'graph TB\n    subgraph "VLA Pipeline"\n        A[Language Input<br/>Natural Language Command] --\x3e B[Language Encoder<br/>BERT/Transformer]\n        C[Visual Input<br/>Camera/Image Data] --\x3e D[Vision Encoder<br/>ResNet/ViT]\n\n        B --\x3e E[Fusion Layer<br/>Cross-Attention]\n        D --\x3e E\n\n        E --\x3e F[Task Understanding<br/>Intent Recognition]\n        F --\x3e G[Action Planning<br/>Sequence Generation]\n        G --\x3e H[Motor Control<br/>Robot Commands]\n\n        I[Environment<br/>Real World] --\x3e C\n        H --\x3e J[Robot Actuators<br/>Physical Action]\n        J --\x3e I\n    end\n'})})}),"\n",(0,a.jsx)(e.h2,{id:"core-components",children:"Core Components"}),"\n",(0,a.jsx)(e.h3,{id:"1-visual-perception-module",children:"1. Visual Perception Module"}),"\n",(0,a.jsx)(e.p,{children:"Responsible for understanding the visual environment and extracting relevant information."}),"\n",(0,a.jsx)(i.A,{title:"Visual Perception System",language:"python",children:(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport numpy as np\nfrom PIL import Image\nimport cv2\n\nclass VisualPerceptionModule(nn.Module):\n    \"\"\"Visual perception for VLA systems\"\"\"\n\n    def __init__(self, feature_dim=512):\n        super().__init__()\n\n        # Backbone CNN for feature extraction\n        self.backbone = models.resnet50(pretrained=True)\n        self.backbone.fc = nn.Identity()  # Remove classification layer\n\n        # Object detection head\n        self.detector_head = nn.Sequential(\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 80)  # COCO classes\n        )\n\n        # Depth estimation head\n        self.depth_head = nn.Sequential(\n            nn.Conv2d(2048, 512, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 256, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 1, 1)\n        )\n\n        # Semantic segmentation head\n        self.segmentation_head = nn.Sequential(\n            nn.Conv2d(2048, 512, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 256, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 19, 1)  # 19 semantic classes\n        )\n\n        # Visual feature projection\n        self.visual_projection = nn.Linear(2048, feature_dim)\n\n    def forward(self, x):\n        # Extract features\n        features = self.backbone(x)  # [batch, 2048]\n\n        # Object detection\n        class_logits = self.detector_head(features)\n\n        # For depth and segmentation, we need feature maps\n        feature_maps = self.backbone.layer4(x)  # [batch, 2048, H, W]\n\n        # Depth estimation\n        depth = self.depth_head(feature_maps)\n\n        # Semantic segmentation\n        segmentation = self.segmentation_head(feature_maps)\n\n        # Project features for fusion\n        visual_features = self.visual_projection(features)\n\n        return {\n            'features': visual_features,\n            'class_logits': class_logits,\n            'depth': depth,\n            'segmentation': segmentation\n        }\n\nclass ObjectGrounding:\n    \"\"\"Ground language concepts to visual objects\"\"\"\n\n    def __init__(self, perception_module):\n        self.perception = perception_module\n        self.object_categories = [\n            'cup', 'bottle', 'book', 'phone', 'laptop',\n            'apple', 'banana', 'plate', 'fork', 'spoon',\n            'chair', 'table', 'door', 'window', 'lamp'\n        ]\n\n    def find_objects(self, image, description):\n        \"\"\"Find objects matching description in image\"\"\"\n        # Process image\n        with torch.no_grad():\n            outputs = self.perception(image.unsqueeze(0))\n\n        # Parse description for object types\n        target_objects = self.parse_objects(description)\n\n        # Find matching objects\n        found_objects = []\n\n        # Use segmentation to locate objects\n        segmentation = outputs['segmentation'].squeeze()\n\n        for obj_class in target_objects:\n            if obj_class in self.object_categories:\n                mask = (segmentation == self.object_categories.index(obj_class))\n                if mask.any():\n                    # Get bounding boxes\n                    boxes = self.get_bounding_boxes(mask)\n                    found_objects.append({\n                        'class': obj_class,\n                        'boxes': boxes,\n                        'center': self.get_center_point(mask)\n                    })\n\n        return found_objects\n\n    def parse_objects(self, description):\n        \"\"\"Extract object types from natural language description\"\"\"\n        # Simple keyword-based extraction (in practice, use NLP)\n        objects = []\n        for obj in self.object_categories:\n            if obj in description.lower():\n                objects.append(obj)\n        return objects\n\nclass SpatialReasoning:\n    \"\"\"Spatial reasoning for object relationships\"\"\"\n\n    def __init__(self):\n        self.spatial_relations = [\n            'on', 'in', 'under', 'beside', 'above', 'below',\n            'left of', 'right of', 'front of', 'behind', 'near', 'far'\n        ]\n\n    def compute_spatial_relations(self, objects, scene_depth):\n        \"\"\"Compute spatial relationships between objects\"\"\"\n        relations = {}\n\n        for i, obj1 in enumerate(objects):\n            relations[obj1['class']] = {}\n\n            for j, obj2 in enumerate(objects):\n                if i != j:\n                    relation = self.get_relation(\n                        obj1, obj2, scene_depth\n                    )\n                    relations[obj1['class']][obj2['class']] = relation\n\n        return relations\n\n    def get_relation(self, obj1, obj2, depth):\n        \"\"\"Determine spatial relationship between two objects\"\"\"\n        pos1 = obj1['center']\n        pos2 = obj2['center']\n\n        # Get depth values at object positions\n        z1 = depth[pos1[1], pos1[0]]\n        z2 = depth[pos2[1], pos2[0]]\n\n        # Compute spatial relationships\n        relations = []\n\n        # Horizontal relationships\n        if pos1[0] < pos2[0] - 50:\n            relations.append('left of')\n        elif pos1[0] > pos2[0] + 50:\n            relations.append('right of')\n\n        # Vertical relationships\n        if pos1[1] < pos2[1] - 50:\n            relations.append('above')\n        elif pos1[1] > pos2[1] + 50:\n            relations.append('below')\n\n        # Depth relationships\n        if abs(z1 - z2) > 0.1:\n            if z1 < z2:\n                relations.append('in front of')\n            else:\n                relations.append('behind')\n\n        # Proximity\n        distance = np.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)\n        if distance < 100:\n            relations.append('near')\n        elif distance > 300:\n            relations.append('far')\n\n        return relations[0] if relations else 'beside'\n"})})}),"\n",(0,a.jsx)(e.h3,{id:"2-language-understanding-module",children:"2. Language Understanding Module"}),"\n",(0,a.jsx)(e.p,{children:"Processes natural language commands and extracts task-relevant information."}),"\n",(0,a.jsx)(i.A,{title:"Language Understanding System",language:"python",children:(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nimport spacy\nimport nltk\n\nclass LanguageUnderstanding(nn.Module):\n    \"\"\"Language understanding module for VLA systems\"\"\"\n\n    def __init__(self, feature_dim=512):\n        super().__init__()\n\n        # Pre-trained language model\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n\n        # Task classification head\n        self.task_classifier = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Linear(256, 10)  # 10 task types\n        )\n\n        # Object extraction head\n        self.object_extractor = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Linear(256, 100)  # 100 possible objects\n        )\n\n        # Spatial relation head\n        self.relation_head = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Linear(256, 20)  # 20 spatial relations\n        )\n\n        # Language feature projection\n        self.lang_projection = nn.Linear(768, feature_dim)\n\n        # NLP processor for structured extraction\n        self.nlp = spacy.load('en_core_web_sm')\n\n    def forward(self, text):\n        # Tokenize text\n        inputs = self.tokenizer(\n            text,\n            return_tensors='pt',\n            padding=True,\n            truncation=True\n        )\n\n        # Get BERT embeddings\n        with torch.no_grad():\n            outputs = self.bert(**inputs)\n\n        # Use [CLS] token as sentence embedding\n        sentence_embedding = outputs.last_hidden_state[:, 0, :]\n\n        # Classify task type\n        task_logits = self.task_classifier(sentence_embedding)\n\n        # Extract objects\n        object_logits = self.object_extractor(sentence_embedding)\n\n        # Identify spatial relations\n        relation_logits = self.relation_head(sentence_embedding)\n\n        # Project for fusion\n        lang_features = self.lang_projection(sentence_embedding)\n\n        return {\n            'features': lang_features,\n            'task_logits': task_logits,\n            'object_logits': object_logits,\n            'relation_logits': relation_logits\n        }\n\n    def extract_structured_info(self, command):\n        \"\"\"Extract structured information from natural language\"\"\"\n        doc = self.nlp(command)\n\n        # Extract task verb\n        task_verb = None\n        for token in doc:\n            if token.pos_ == 'VERB':\n                task_verb = token.lemma_\n                break\n\n        # Extract objects (nouns)\n        objects = []\n        for chunk in doc.noun_chunks:\n            if chunk.root.pos_ == 'NOUN':\n                objects.append(chunk.text)\n\n        # Extract spatial relations\n        relations = []\n        for token in doc:\n            if token.text.lower() in ['on', 'in', 'under', 'beside', 'above']:\n                relations.append(token.text.lower())\n\n        # Extract colors and attributes\n        attributes = []\n        for token in doc:\n            if token.pos_ == 'ADJ':\n                attributes.append(token.text)\n\n        return {\n            'task': task_verb,\n            'objects': objects,\n            'relations': relations,\n            'attributes': attributes\n        }\n\nclass TaskPlanner:\n    \"\"\"Plan robot actions from language commands\"\"\"\n\n    def __init__(self):\n        self.action_primitives = [\n            'move_to', 'pick_up', 'place', 'open', 'close',\n            'push', 'pull', 'grasp', 'release', 'navigate'\n        ]\n\n        # Task templates\n        self.task_templates = {\n            'pick': ['move_to', 'grasp', 'lift'],\n            'place': ['move_to', 'release'],\n            'bring': ['move_to', 'grasp', 'lift', 'move_to', 'release'],\n            'clean': ['move_to', 'grasp', 'clean', 'release'],\n            'find': ['search', 'identify']\n        }\n\n    def plan_from_command(self, structured_command, scene_info):\n        \"\"\"Generate action sequence from command\"\"\"\n        task = structured_command['task']\n        objects = structured_command['objects']\n        relations = structured_command['relations']\n\n        # Get task template\n        if task in self.task_templates:\n            action_sequence = self.task_templates[task].copy()\n        else:\n            # Default planning\n            action_sequence = ['move_to', 'grasp', 'move_to', 'release']\n\n        # Fill in action parameters\n        planned_actions = []\n\n        for action in action_sequence:\n            action_params = {'type': action}\n\n            # Add object references\n            if objects:\n                action_params['object'] = objects[0]\n\n            # Add spatial constraints\n            if relations:\n                action_params['relation'] = relations[0]\n\n            # Add location from scene understanding\n            if 'object' in action_params and scene_info:\n                target_object = action_params['object']\n                if target_object in scene_info:\n                    action_params['location'] = scene_info[target_object]['location']\n\n            planned_actions.append(action_params)\n\n        return planned_actions\n\nclass ExecutionController:\n    \"\"\"Control robot execution of planned actions\"\"\"\n\n    def __init__(self, robot_interface):\n        self.robot = robot_interface\n        self.action_map = {\n            'move_to': self.execute_move,\n            'grasp': self.execute_grasp,\n            'release': self.execute_release,\n            'lift': self.execute_lift,\n            'push': self.execute_push\n        }\n\n    def execute_plan(self, action_sequence):\n        \"\"\"Execute a sequence of actions\"\"\"\n        results = []\n\n        for action in action_sequence:\n            action_type = action['type']\n\n            if action_type in self.action_map:\n                result = self.action_map[action_type](action)\n                results.append(result)\n\n                if not result['success']:\n                    # Stop execution on failure\n                    break\n            else:\n                results.append({\n                    'success': False,\n                    'error': f'Unknown action type: {action_type}'\n                })\n\n        return results\n\n    def execute_move(self, params):\n        \"\"\"Execute move action\"\"\"\n        target = params.get('location', None)\n\n        if target is None:\n            return {'success': False, 'error': 'No target location'}\n\n        # Move robot to target\n        success = self.robot.move_to(target)\n\n        return {\n            'success': success,\n            'action': 'move_to',\n            'target': target\n        }\n\n    def execute_grasp(self, params):\n        \"\"\"Execute grasp action\"\"\"\n        object_name = params.get('object', None)\n\n        # Get object position\n        object_pose = self.robot.get_object_pose(object_name)\n\n        if object_pose is None:\n            return {'success': False, 'error': f'Object {object_name} not found'}\n\n        # Move to object\n        self.robot.move_to(object_pose)\n\n        # Grasp object\n        success = self.robot.grasp()\n\n        return {\n            'success': success,\n            'action': 'grasp',\n            'object': object_name\n        }\n\n    def execute_release(self, params):\n        \"\"\"Execute release action\"\"\"\n        success = self.robot.release()\n\n        return {\n            'success': success,\n            'action': 'release'\n        }\n"})})}),"\n",(0,a.jsx)(e.h3,{id:"3-vision-language-fusion",children:"3. Vision-Language Fusion"}),"\n",(0,a.jsx)(e.p,{children:"Combines visual and language representations for task understanding."}),"\n",(0,a.jsx)(i.A,{title:"Cross-Attention Fusion Module",language:"python",children:(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CrossAttentionFusion(nn.Module):\n    \"\"\"Cross-attention fusion for vision and language features\"\"\"\n\n    def __init__(self, feature_dim=512, num_heads=8):\n        super().__init__()\n\n        self.feature_dim = feature_dim\n        self.num_heads = num_heads\n\n        # Multi-head attention\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=num_heads,\n            batch_first=True\n        )\n\n        # Feed-forward network\n        self.ffn = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim * 4),\n            nn.ReLU(),\n            nn.Linear(feature_dim * 4, feature_dim)\n        )\n\n        # Layer normalization\n        self.norm1 = nn.LayerNorm(feature_dim)\n        self.norm2 = nn.LayerNorm(feature_dim)\n\n        # Task-specific heads\n        self.action_head = nn.Sequential(\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 20)  # 20 action types\n        )\n\n        self.value_head = nn.Sequential(\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, visual_features, lang_features):\n        \"\"\"\n        Args:\n            visual_features: [batch, num_regions, feature_dim]\n            lang_features: [batch, seq_len, feature_dim]\n        \"\"\"\n\n        # Cross-attention: language queries, visual keys/values\n        attended_features, attention_weights = self.cross_attention(\n            query=lang_features,\n            key=visual_features,\n            value=visual_features\n        )\n\n        # Residual connection and normalization\n        attended_features = self.norm1(lang_features + attended_features)\n\n        # Feed-forward network\n        ffn_output = self.ffn(attended_features)\n\n        # Residual connection and normalization\n        fused_features = self.norm2(attended_features + ffn_output)\n\n        # Global pooling for classification\n        pooled_features = fused_features.mean(dim=1)\n\n        # Generate actions\n        action_logits = self.action_head(pooled_features)\n\n        # Generate value function\n        value = self.value_head(pooled_features)\n\n        return {\n            'fused_features': fused_features,\n            'attention_weights': attention_weights,\n            'action_logits': action_logits,\n            'value': value\n        }\n\nclass EmbodiedAgent(nn.Module):\n    \"\"\"Complete embodied agent for VLA tasks\"\"\"\n\n    def __init__(self, feature_dim=512):\n        super().__init__()\n\n        # Vision module\n        self.vision_module = VisualPerceptionModule(feature_dim)\n\n        # Language module\n        self.lang_module = LanguageUnderstanding(feature_dim)\n\n        # Fusion module\n        self.fusion_module = CrossAttentionFusion(feature_dim)\n\n        # Action decoder\n        self.action_decoder = nn.LSTM(\n            input_size=feature_dim,\n            hidden_size=512,\n            num_layers=2,\n            batch_first=True\n        )\n\n        # Action prediction\n        self.action_predictor = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 20)  # 20 action primitives\n        )\n\n        # Position prediction\n        self.position_predictor = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 3)  # x, y, z position\n        )\n\n        # Gripper state prediction\n        self.gripper_predictor = nn.Sequential(\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)  # gripper open (0) or closed (1)\n        )\n\n    def forward(self, image, instruction):\n        # Extract visual features\n        visual_outputs = self.vision_module(image)\n        visual_features = visual_outputs['features']\n\n        # Extract language features\n        lang_outputs = self.lang_module(instruction)\n        lang_features = lang_outputs['features']\n\n        # Expand visual features for attention\n        # Assume we have visual features for different regions\n        num_regions = 10  # Example: 10 salient regions\n        visual_features_expanded = visual_features.unsqueeze(1).repeat(1, num_regions, 1)\n\n        # Fuse vision and language\n        fusion_outputs = self.fusion_module(\n            visual_features_expanded,\n            lang_features.unsqueeze(1)\n        )\n\n        fused_features = fusion_outputs['fused_features'].mean(dim=1)\n\n        # Generate action sequence\n        # Initialize LSTM with fused features\n        lstm_out, _ = self.action_decoder(fused_features.unsqueeze(0))\n\n        # Predict actions\n        actions = self.action_predictor(lstm_out)\n        positions = self.position_predictor(lstm_out)\n        gripper_states = torch.sigmoid(self.gripper_predictor(lstm_out))\n\n        return {\n            'actions': actions,\n            'positions': positions,\n            'gripper_states': gripper_states,\n            'attention_weights': fusion_outputs['attention_weights']\n        }\n"})})}),"\n",(0,a.jsx)(e.h2,{id:"training-vla-models",children:"Training VLA Models"}),"\n",(0,a.jsx)(e.h3,{id:"data-collection-and-preprocessing",children:"Data Collection and Preprocessing"}),"\n",(0,a.jsx)(i.A,{title:"VLA Training Pipeline",language:"python",children:(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport json\nimport cv2\nfrom PIL import Image\n\nclass VLADataset(Dataset):\n    \"\"\"Dataset for VLA training\"\"\"\n\n    def __init__(self, data_path):\n        self.data = self.load_data(data_path)\n        self.transform = self.get_transforms()\n\n    def load_data(self, path):\n        \"\"\"Load VLA training data\"\"\"\n        with open(path, 'r') as f:\n            return json.load(f)\n\n    def get_transforms(self):\n        \"\"\"Image preprocessing transforms\"\"\"\n        from torchvision import transforms\n\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n\n        # Load image\n        image = Image.open(item['image_path']).convert('RGB')\n        image = self.transform(image)\n\n        # Get instruction\n        instruction = item['instruction']\n\n        # Get target actions\n        actions = torch.tensor(item['actions'], dtype=torch.long)\n        positions = torch.tensor(item['positions'], dtype=torch.float)\n        gripper_states = torch.tensor(item['gripper_states'], dtype=torch.float)\n\n        return {\n            'image': image,\n            'instruction': instruction,\n            'actions': actions,\n            'positions': positions,\n            'gripper_states': gripper_states\n        }\n\nclass VLATrainer:\n    \"\"\"Trainer for VLA models\"\"\"\n\n    def __init__(self, model, device='cuda'):\n        self.model = model.to(device)\n        self.device = device\n\n        # Loss functions\n        self.action_loss = nn.CrossEntropyLoss()\n        self.position_loss = nn.MSELoss()\n        self.gripper_loss = nn.BCELoss()\n\n        # Optimizer\n        self.optimizer = torch.optim.Adam(\n            model.parameters(),\n            lr=1e-4,\n            weight_decay=1e-5\n        )\n\n        # Learning rate scheduler\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer,\n            mode='min',\n            patience=5\n        )\n\n    def train_epoch(self, dataloader):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        total_loss = 0\n\n        for batch in dataloader:\n            # Move batch to device\n            image = batch['image'].to(self.device)\n            instruction = batch['instruction']\n            actions = batch['actions'].to(self.device)\n            positions = batch['positions'].to(self.device)\n            gripper_states = batch['gripper_states'].to(self.device)\n\n            # Forward pass\n            outputs = self.model(image, instruction)\n\n            # Calculate losses\n            loss_action = self.action_loss(\n                outputs['actions'].view(-1, 20),\n                actions.view(-1)\n            )\n\n            loss_position = self.position_loss(\n                outputs['positions'],\n                positions\n            )\n\n            loss_gripper = self.gripper_loss(\n                outputs['gripper_states'],\n                gripper_states\n            )\n\n            # Total loss\n            total_loss_batch = (\n                loss_action +\n                0.5 * loss_position +\n                0.5 * loss_gripper\n            )\n\n            # Backward pass\n            self.optimizer.zero_grad()\n            total_loss_batch.backward()\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n\n            self.optimizer.step()\n\n            total_loss += total_loss_batch.item()\n\n        return total_loss / len(dataloader)\n\n    def evaluate(self, dataloader):\n        \"\"\"Evaluate model\"\"\"\n        self.model.eval()\n        total_loss = 0\n        correct_actions = 0\n        total_actions = 0\n\n        with torch.no_grad():\n            for batch in dataloader:\n                image = batch['image'].to(self.device)\n                instruction = batch['instruction']\n                actions = batch['actions'].to(self.device)\n                positions = batch['positions'].to(self.device)\n                gripper_states = batch['gripper_states'].to(self.device)\n\n                outputs = self.model(image, instruction)\n\n                # Calculate losses\n                loss_action = self.action_loss(\n                    outputs['actions'].view(-1, 20),\n                    actions.view(-1)\n                )\n\n                loss_position = self.position_loss(\n                    outputs['positions'],\n                    positions\n                )\n\n                loss_gripper = self.gripper_loss(\n                    outputs['gripper_states'],\n                    gripper_states\n                )\n\n                total_loss_batch = (\n                    loss_action +\n                    0.5 * loss_position +\n                    0.5 * loss_gripper\n                )\n\n                total_loss += total_loss_batch.item()\n\n                # Calculate accuracy\n                pred_actions = torch.argmax(outputs['actions'], dim=-1)\n                correct_actions += (pred_actions == actions).sum().item()\n                total_actions += actions.numel()\n\n        avg_loss = total_loss / len(dataloader)\n        accuracy = correct_actions / total_actions\n\n        return avg_loss, accuracy\n\ndef generate_synthetic_data(num_samples=1000):\n    \"\"\"Generate synthetic VLA training data\"\"\"\n    import random\n\n    data = []\n\n    # Object templates\n    objects = ['cup', 'bottle', 'book', 'phone', 'laptop', 'apple', 'banana']\n    colors = ['red', 'blue', 'green', 'yellow', 'black', 'white']\n    locations = ['table', 'desk', 'shelf', 'counter', 'floor']\n\n    # Action templates\n    action_templates = {\n        'pick': ['Pick up the {color} {object}', 'Get the {object}'],\n        'place': ['Place the {object} on the {location}', 'Put the {object} here'],\n        'bring': ['Bring me the {color} {object}', 'Get the {object} for me'],\n        'clean': ['Clean the {location}', 'Wipe the {object}']\n    }\n\n    for i in range(num_samples):\n        # Randomly choose action and objects\n        action = random.choice(list(action_templates.keys()))\n        obj = random.choice(objects)\n        color = random.choice(colors)\n        location = random.choice(locations)\n\n        # Generate instruction\n        template = random.choice(action_templates[action])\n        instruction = template.format(\n            object=obj,\n            color=color,\n            location=location\n        )\n\n        # Generate synthetic actions\n        if action == 'pick':\n            actions = [1, 5, 3]  # move_to, grasp, lift\n        elif action == 'place':\n            actions = [1, 7]  # move_to, release\n        elif action == 'bring':\n            actions = [1, 5, 3, 1, 7]  # move_to, grasp, lift, move_to, release\n        else:\n            actions = [1, 2]  # move_to, clean\n\n        # Generate synthetic positions\n        positions = torch.randn(len(actions), 3) * 0.5\n\n        # Generate gripper states\n        gripper_states = torch.zeros(len(actions), 1)\n        if 5 in actions:  # grasp action\n            gripper_states[actions.index(5):] = 1.0\n        if 7 in actions:  # release action\n            gripper_states[actions.index(7):] = 0.0\n\n        # Create data item\n        data.append({\n            'image_path': f'images/sample_{i}.jpg',\n            'instruction': instruction,\n            'actions': actions,\n            'positions': positions.tolist(),\n            'gripper_states': gripper_states.tolist()\n        })\n\n    return data\n"})})}),"\n",(0,a.jsx)(e.h2,{id:"lab-exercise-building-a-vla-powered-assistant",children:"Lab Exercise: Building a VLA-Powered Assistant"}),"\n",(0,a.jsxs)("div",{className:"lab-exercise",children:[(0,a.jsx)(e.h3,{id:"objective",children:"Objective"}),(0,a.jsx)(e.p,{children:"Create a complete VLA system that can understand natural language commands and execute them in a simulated environment."}),(0,a.jsx)(e.h3,{id:"setup",children:"Setup"}),(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Isaac Sim for simulation"}),"\n",(0,a.jsx)(e.li,{children:"Custom VLA model implementation"}),"\n",(0,a.jsx)(e.li,{children:"Dataset generation pipeline"}),"\n",(0,a.jsx)(e.li,{children:"Training and evaluation framework"}),"\n"]}),(0,a.jsx)(e.h3,{id:"implementation",children:"Implementation"}),(0,a.jsx)(i.A,{language:"python",editable:!0,children:(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# vla_assistant.py\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nclass VLAAssistant:\n    \"\"\"VLA-powered robot assistant\"\"\"\n\n    def __init__(self, model_path=None):\n        # Initialize components\n        self.perception = VisualPerceptionModule()\n        self.lang_understanding = LanguageUnderstanding()\n        self.fusion = CrossAttentionFusion()\n        self.planner = TaskPlanner()\n\n        # Load trained model if available\n        if model_path:\n            self.load_model(model_path)\n\n        # Robot interface (simulated)\n        self.robot = SimulatedRobot()\n\n        # Conversation memory\n        self.memory = []\n\n    def process_command(self, instruction, image):\n        \"\"\"Process a natural language command\"\"\"\n        # Store in memory\n        self.memory.append({\n            'instruction': instruction,\n            'timestamp': time.time()\n        })\n\n        # Step 1: Understand the command\n        structured_cmd = self.lang_understanding.extract_structured_info(instruction)\n        print(f\"Understanding: {structured_cmd}\")\n\n        # Step 2: Perceive the environment\n        scene_info = self.perceive_scene(image)\n        print(f\"Scene contains: {list(scene_info.keys())}\")\n\n        # Step 3: Ground language to perception\n        grounded_objects = self.ground_language_to_vision(\n            structured_cmd['objects'], scene_info\n        )\n\n        # Step 4: Plan actions\n        action_plan = self.planner.plan_from_command(structured_cmd, grounded_objects)\n        print(f\"Action plan: {action_plan}\")\n\n        # Step 5: Execute actions\n        results = self.execute_actions(action_plan)\n\n        return results\n\n    def perceive_scene(self, image):\n        \"\"\"Analyze the scene and extract object information\"\"\"\n        # Process image\n        with torch.no_grad():\n            outputs = self.perception(image.unsqueeze(0))\n\n        # Extract objects from segmentation\n        segmentation = outputs['segmentation'].squeeze()\n        depth = outputs['depth'].squeeze()\n\n        # Find objects\n        scene_info = {}\n\n        # Simple object detection (in practice, use more sophisticated method)\n        objects = [\n            {'name': 'cup', 'color': 'red', 'pos': (100, 200)},\n            {'name': 'bottle', 'color': 'blue', 'pos': (300, 150)},\n            {'name': 'book', 'color': 'green', 'pos': (200, 300)}\n        ]\n\n        for obj in objects:\n            scene_info[f\"{obj['color']} {obj['name']}\"] = {\n                'location': obj['pos'],\n                'depth': depth[obj['pos'][1], obj['pos'][0]],\n                'bounding_box': self.get_bbox_at(obj['pos'], segmentation)\n            }\n\n        return scene_info\n\n    def ground_language_to_vision(self, objects, scene_info):\n        \"\"\"Ground language objects to visual detections\"\"\"\n        grounded = {}\n\n        for obj in objects:\n            # Find matching object in scene\n            for scene_obj, info in scene_info.items():\n                if obj.lower() in scene_obj.lower():\n                    grounded[obj] = info\n                    break\n\n        return grounded\n\n    def execute_actions(self, action_plan):\n        \"\"\"Execute the planned actions\"\"\"\n        results = []\n\n        for i, action in enumerate(action_plan):\n            print(f\"Executing action {i+1}: {action}\")\n\n            if action['type'] == 'move_to':\n                result = self.robot.move_to(action['location'])\n            elif action['type'] == 'grasp':\n                result = self.robot.grasp()\n            elif action['type'] == 'release':\n                result = self.robot.release()\n            else:\n                result = {'success': False, 'error': 'Unknown action'}\n\n            results.append(result)\n\n            # Visualize action\n            self.visualize_action(action, result)\n\n            if not result['success']:\n                print(f\"Action failed: {result.get('error')}\")\n                break\n\n        return results\n\n    def visualize_action(self, action, result):\n        \"\"\"Visualize robot action\"\"\"\n        # Create visualization\n        plt.figure(figsize=(10, 5))\n\n        # Current robot state\n        plt.subplot(1, 2, 1)\n        self.visualize_robot_state()\n        plt.title(f\"Action: {action['type']}\")\n\n        # Result\n        plt.subplot(1, 2, 2)\n        if result['success']:\n            plt.text(0.5, 0.5, 'SUCCESS',\n                    ha='center', va='center',\n                    fontsize=20, color='green',\n                    transform=plt.gca().transAxes)\n        else:\n            plt.text(0.5, 0.5, f\"FAILED\\n{result.get('error', '')}\",\n                    ha='center', va='center',\n                    fontsize=14, color='red',\n                    transform=plt.gca().transAxes)\n\n        plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n\n    def demonstrate_capabilities(self):\n        \"\"\"Demonstrate VLA capabilities\"\"\"\n        print(\"=== VLA Assistant Demo ===\")\n\n        # Test commands\n        test_commands = [\n            \"Pick up the red cup\",\n            \"Bring me the blue bottle\",\n            \"Place the book on the table\",\n            \"Clean the area\"\n        ]\n\n        for command in test_commands:\n            print(f\"\\nCommand: {command}\")\n            print(\"-\" * 40)\n\n            # Simulate getting an image\n            test_image = self.create_test_scene()\n\n            # Process command\n            results = self.process_command(command, test_image)\n\n            # Report results\n            success = all(r['success'] for r in results)\n            print(f\"Result: {'SUCCESS' if success else 'FAILED'}\")\n\n            time.sleep(1)  # Pause between commands\n\nclass SimulatedRobot:\n    \"\"\"Simulated robot for testing\"\"\"\n\n    def __init__(self):\n        self.position = np.array([0.0, 0.0, 0.0])\n        self.gripper_closed = False\n        self.holding = None\n\n    def move_to(self, location):\n        \"\"\"Move to specified location\"\"\"\n        if isinstance(location, tuple):\n            target = np.array([location[0], location[1], 0.0])\n        else:\n            target = np.array(location)\n\n        # Simulate movement\n        self.position = target\n\n        return {\n            'success': True,\n            'new_position': self.position.tolist()\n        }\n\n    def grasp(self):\n        \"\"\"Close gripper to grasp object\"\"\"\n        if not self.gripper_closed:\n            self.gripper_closed = True\n            self.holding = 'object'  # Simulate holding something\n            return {'success': True}\n        else:\n            return {'success': False, 'error': 'Gripper already closed'}\n\n    def release(self):\n        \"\"\"Open gripper to release object\"\"\"\n        if self.gripper_closed:\n            self.gripper_closed = False\n            self.holding = None\n            return {'success': True}\n        else:\n            return {'success': False, 'error': 'Gripper already open'}\n\n# Demo\nif __name__ == \"__main__\":\n    assistant = VLAAssistant()\n    assistant.demonstrate_capabilities()\n"})})}),(0,a.jsx)(e.h3,{id:"training-the-model",children:"Training the Model"}),(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Generate training data"}),":"]}),"\n"]}),(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Create synthetic dataset\ntrain_data = generate_synthetic_data(num_samples=5000)\ntest_data = generate_synthetic_data(num_samples=1000)\n\n# Save datasets\nwith open('train_data.json', 'w') as f:\n    json.dump(train_data, f)\nwith open('test_data.json', 'w') as f:\n    json.dump(test_data, f)\n"})}),(0,a.jsxs)(e.ol,{start:"2",children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Train the VLA model"}),":"]}),"\n"]}),(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Initialize model and trainer\nmodel = EmbodiedAgent()\ntrainer = VLATrainer(model)\n\n# Create datasets\ntrain_dataset = VLADataset('train_data.json')\ntest_dataset = VLADataset('test_data.json')\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Training loop\nbest_loss = float('inf')\nfor epoch in range(50):\n    train_loss = trainer.train_epoch(train_loader)\n    val_loss, val_acc = trainer.evaluate(test_loader)\n\n    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, \"\n          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n\n    # Save best model\n    if val_loss < best_loss:\n        best_loss = val_loss\n        torch.save(model.state_dict(), 'best_vla_model.pth')\n"})}),(0,a.jsx)(e.h3,{id:"expected-results",children:"Expected Results"}),(0,a.jsx)(e.p,{children:"The VLA assistant should demonstrate:"}),(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding of natural language commands"}),"\n",(0,a.jsx)(e.li,{children:"Visual object recognition and localization"}),"\n",(0,a.jsx)(e.li,{children:"Action planning and execution"}),"\n",(0,a.jsx)(e.li,{children:"Error handling and recovery"}),"\n",(0,a.jsx)(e.li,{children:"Conversational interaction"}),"\n"]})]}),"\n",(0,a.jsx)(e.h2,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,a.jsx)(e.h3,{id:"1-home-assistants",children:"1. Home Assistants"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:'"Clean up the toys on the floor"'}),"\n",(0,a.jsx)(e.li,{children:'"Bring me a glass of water"'}),"\n",(0,a.jsx)(e.li,{children:'"Set the table for dinner"'}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-manufacturing",children:"2. Manufacturing"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:'"Assemble the red components"'}),"\n",(0,a.jsx)(e.li,{children:'"Inspect the quality of products"'}),"\n",(0,a.jsx)(e.li,{children:'"Package the items in boxes"'}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-healthcare",children:"3. Healthcare"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:'"Bring the medication to room 302"'}),"\n",(0,a.jsx)(e.li,{children:'"Disinfect the bed area"'}),"\n",(0,a.jsx)(e.li,{children:'"Assist the patient with walking"'}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(e.h3,{id:"1-data-quality",children:"1. Data Quality"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Collect diverse training data"}),"\n",(0,a.jsx)(e.li,{children:"Include various environments and lighting"}),"\n",(0,a.jsx)(e.li,{children:"Ensure balanced action distribution"}),"\n",(0,a.jsx)(e.li,{children:"Use domain randomization"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-model-architecture",children:"2. Model Architecture"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use pre-trained vision and language models"}),"\n",(0,a.jsx)(e.li,{children:"Implement attention mechanisms"}),"\n",(0,a.jsx)(e.li,{children:"Include safety constraints"}),"\n",(0,a.jsx)(e.li,{children:"Design for interpretability"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-deployment",children:"3. Deployment"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Test extensively in simulation first"}),"\n",(0,a.jsx)(e.li,{children:"Implement safety checks"}),"\n",(0,a.jsx)(e.li,{children:"Monitor for unexpected behaviors"}),"\n",(0,a.jsx)(e.li,{children:"Provide override mechanisms"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"VLA systems bridge perception and action"})," through language understanding"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Cross-attention mechanisms"})," effectively fuse vision and language"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Synthetic data generation"})," accelerates VLA model training"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Hierarchical planning"})," breaks complex tasks into action primitives"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety is paramount"})," when deploying VLA systems in the real world"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Vision-Language-Action systems represent the next frontier in robotics, enabling robots to understand and execute natural language commands in complex environments. By combining state-of-the-art computer vision, natural language processing, and control systems, VLA models create the foundation for truly intelligent robotic assistants that can collaborate naturally with humans."}),"\n",(0,a.jsxs)(e.p,{children:["In the next lesson, we'll explore ",(0,a.jsx)(e.strong,{children:"Conversational Robots"})," that can maintain dialogue and learn from interaction."]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.a,{href:"lesson-3",children:"Next: Conversational Robots \u2192"})}),"\n",(0,a.jsx)(e.h2,{id:"quiz",children:"Quiz"}),"\n",(0,a.jsx)(s,{quizId:"vision-language-action-systems",questions:[{id:"q1",type:"multiple-choice",question:"What is the primary purpose of cross-attention in VLA systems?",options:["To speed up model training","To fuse vision and language features","To reduce model size","To improve image quality"],correct:1,explanation:"Cross-attention mechanisms in VLA systems are used to effectively fuse vision and language features, allowing the model to ground language concepts in visual perception."},{id:"q2",type:"multiple-choice",question:"Which component is responsible for converting natural language to action sequences?",options:["Visual Perception Module","Fusion Module","Task Planner","Execution Controller"],correct:2,explanation:"The Task Planner is responsible for understanding the structured information extracted from natural language and generating appropriate action sequences for the robot to execute."},{id:"q3",type:"true-false",question:"VLA systems require labeled datasets where every image has corresponding language commands and action sequences.",correct:!0,explanation:"VLA systems are typically trained on datasets that contain triplets of (image, language command, action sequence), making them data-intensive compared to single-modality models."}]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(u,{...n})}):u(n)}function m(n,e){throw new Error("Expected "+(e?"component":"object")+" `"+n+"` to be defined: you likely forgot to import, pass, or provide it.")}},8453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>r});var s=t(6540);const a={},o=s.createContext(a);function i(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);